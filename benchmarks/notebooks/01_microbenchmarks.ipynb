{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aecd9ff",
   "metadata": {},
   "source": [
    "# Microbenchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from benchmarks.runner import BenchmarkRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690888a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chis = [8, 16, 32, 64, 128]\n",
    "reps = 5\n",
    "svd_results = []\n",
    "for chi in chis:\n",
    "    runner = BenchmarkRunner()\n",
    "    times = []\n",
    "    mems = []\n",
    "    for _ in range(reps):\n",
    "        mat = np.random.randn(chi, chi)\n",
    "        rec = runner.run_multiple(mat, lambda m: np.linalg.svd(m, full_matrices=False), repetitions=3)\n",
    "        times.append(rec['run_time'])\n",
    "        mems.append(rec['run_peak_memory'])\n",
    "    svd_results.append({\n",
    "        'chi': chi,\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'mean_mem': np.mean(mems),\n",
    "        'std_mem': np.std(mems)\n",
    "    })\n",
    "svd_df = pd.DataFrame(svd_results)\n",
    "svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(svd_df['chi'], svd_df['mean_time'], marker='o')\n",
    "plt.xlabel(r'$\\chi$')\n",
    "plt.ylabel('Runtime (s)')\n",
    "plt.title('SVD runtime vs bond dimension')\n",
    "plt.grid(True, which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frontiers = [10, 50, 200, 1000]\n",
    "reps = 5\n",
    "dd_results = []\n",
    "for r in frontiers:\n",
    "    runner = BenchmarkRunner()\n",
    "    times = []\n",
    "    mems = []\n",
    "    for _ in range(reps):\n",
    "        data = np.random.rand(r)\n",
    "        rec = runner.run_multiple(data, lambda x: np.sort(x), repetitions=3)\n",
    "        times.append(rec['run_time'])\n",
    "        mems.append(rec['run_peak_memory'])\n",
    "    dd_results.append({\n",
    "        'frontier': r,\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'mean_mem': np.mean(mems),\n",
    "        'std_mem': np.std(mems)\n",
    "    })\n",
    "dd_df = pd.DataFrame(dd_results)\n",
    "dd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(dd_df['frontier'], dd_df['mean_time'], marker='o')\n",
    "plt.xlabel('Frontier size')\n",
    "plt.ylabel('Runtime (s)')\n",
    "plt.title('DD frontier extraction time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = list(range(2, 9))\n",
    "reps = 5\n",
    "st_results = []\n",
    "for w in windows:\n",
    "    runner = BenchmarkRunner()\n",
    "    times = []\n",
    "    mems = []\n",
    "    for _ in range(reps):\n",
    "        rec = runner.run_multiple(w, lambda win: sum(range(1 << win)), repetitions=3)\n",
    "        times.append(rec['run_time'])\n",
    "        mems.append(rec['run_peak_memory'])\n",
    "    st_results.append({\n",
    "        'window': w,\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'mean_mem': np.mean(mems),\n",
    "        'std_mem': np.std(mems)\n",
    "    })\n",
    "st_df = pd.DataFrame(st_results)\n",
    "st_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6808f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from benchmarks.stats_utils import stats_table\n",
    "\n",
    "def add_stats(df, quasar_col='QuASAr', baseline_cols=None, test='ttest', correction='bonferroni'):\n",
    "    \"\"\"Compute statistics comparing QuASAr with baselines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with per-circuit results. One column must correspond to QuASAr,\n",
    "        others to baselines.\n",
    "    quasar_col : str\n",
    "        Name of the column containing QuASAr results.\n",
    "    baseline_cols : list[str] | None\n",
    "        Columns to treat as baselines. Defaults to all columns except quasar_col.\n",
    "    test : str\n",
    "        'ttest' or 'wilcoxon'.\n",
    "    correction : str\n",
    "        'bonferroni' or 'fdr_bh'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Table with baseline name, statistic, corrected p-value, and effect size.\n",
    "    \"\"\"\n",
    "    if baseline_cols is None:\n",
    "        baseline_cols = [c for c in df.columns if c != quasar_col]\n",
    "    baselines = {c: df[c] for c in baseline_cols}\n",
    "    return stats_table(df[quasar_col], baselines, test=test, correction=correction)\n",
    "\n",
    "# Example usage after computing results DataFrame named `results_df`:\n",
    "# stats_df = add_stats(results_df)\n",
    "# stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Record parameters and results\n",
    "import json, pathlib\n",
    "try:\n",
    "    import ipynbname\n",
    "    nb_name = ipynbname.path().stem\n",
    "except Exception:  # pragma: no cover\n",
    "    nb_name = 'notebook'\n",
    "\n",
    "# Collect simple parameters from globals\n",
    "_params = {\n",
    "    k: v for k, v in globals().items()\n",
    "    if not k.startswith('_') and isinstance(v, (int, float, str, bool, list, dict, tuple))\n",
    "}\n",
    "pathlib.Path('../results').mkdir(exist_ok=True)\n",
    "with open(f\"../results/{nb_name}_params.json\", 'w') as f:\n",
    "    json.dump(_params, f, indent=2)\n",
    "if 'results' in globals():\n",
    "    try:\n",
    "        with open(f\"../results/{nb_name}_results.json\", 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "    except TypeError:\n",
    "        pass\n",
    "print(json.dumps(_params, indent=2))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
