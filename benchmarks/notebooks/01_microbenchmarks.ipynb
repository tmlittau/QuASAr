{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aecd9ff",
   "metadata": {},
   "source": [
    "# Microbenchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from benchmarks.runner import BenchmarkRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690888a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chis = [8, 16, 32, 64, 128]\n",
    "reps = 5\n",
    "svd_results = []\n",
    "for chi in chis:\n",
    "    runner = BenchmarkRunner()\n",
    "    times = []\n",
    "    mems = []\n",
    "    for _ in range(reps):\n",
    "        mat = np.random.randn(chi, chi)\n",
    "        rec = runner.run(mat, lambda m: np.linalg.svd(m, full_matrices=False))\n",
    "        times.append(rec['run_time'])\n",
    "        mems.append(rec['run_peak_memory'])\n",
    "    svd_results.append({\n",
    "        'chi': chi,\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'mean_mem': np.mean(mems),\n",
    "        'std_mem': np.std(mems)\n",
    "    })\n",
    "svd_df = pd.DataFrame(svd_results)\n",
    "svd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(svd_df['chi'], svd_df['mean_time'], marker='o')\n",
    "plt.xlabel(r'$\\chi$')\n",
    "plt.ylabel('Runtime (s)')\n",
    "plt.title('SVD runtime vs bond dimension')\n",
    "plt.grid(True, which='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frontiers = [10, 50, 200, 1000]\n",
    "reps = 5\n",
    "dd_results = []\n",
    "for r in frontiers:\n",
    "    runner = BenchmarkRunner()\n",
    "    times = []\n",
    "    mems = []\n",
    "    for _ in range(reps):\n",
    "        data = np.random.rand(r)\n",
    "        rec = runner.run(data, lambda x: np.sort(x))\n",
    "        times.append(rec['run_time'])\n",
    "        mems.append(rec['run_peak_memory'])\n",
    "    dd_results.append({\n",
    "        'frontier': r,\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'mean_mem': np.mean(mems),\n",
    "        'std_mem': np.std(mems)\n",
    "    })\n",
    "dd_df = pd.DataFrame(dd_results)\n",
    "dd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(dd_df['frontier'], dd_df['mean_time'], marker='o')\n",
    "plt.xlabel('Frontier size')\n",
    "plt.ylabel('Runtime (s)')\n",
    "plt.title('DD frontier extraction time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = list(range(2, 9))\n",
    "reps = 5\n",
    "st_results = []\n",
    "for w in windows:\n",
    "    runner = BenchmarkRunner()\n",
    "    times = []\n",
    "    mems = []\n",
    "    for _ in range(reps):\n",
    "        rec = runner.run(w, lambda win: sum(range(1 << win)))\n",
    "        times.append(rec['run_time'])\n",
    "        mems.append(rec['run_peak_memory'])\n",
    "    st_results.append({\n",
    "        'window': w,\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times),\n",
    "        'mean_mem': np.mean(mems),\n",
    "        'std_mem': np.std(mems)\n",
    "    })\n",
    "st_df = pd.DataFrame(st_results)\n",
    "st_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_df"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
