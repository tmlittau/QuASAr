# Benchmark utilities

The benchmarking entry points live in :mod:`benchmarks.bench_utils` with a thin
CLI wrapper in :mod:`benchmarks.run_benchmark`.  The helpers provide two
complementary workflows:

* **Showcase execution** – run the curated benchmark circuits across QuASAr and
  the baseline simulators.  Circuits can be targeted individually, via named
  groups, or by running the entire suite.
* **Theoretical estimation** – evaluate the cost model for the same circuits
  without executing the simulators in order to obtain analytical runtime and
  memory predictions.

Both workflows reuse the shared benchmarking utilities so that the behaviour is
identical between the CLI and programmatic usage.

## Installation

Install the project and optional analysis dependencies:

```bash
pip install -e .[test]
pip install pandas jupyter
```

## Running benchmarks

Launch the showcase suite with the default configuration:

```bash
python benchmarks/run_benchmark.py
```

The command iterates over all showcase circuits defined in
``benchmarks/bench_utils/showcase_benchmarks.py`` and writes the raw results,
summaries and figures to ``benchmarks/bench_utils/results/showcase``.  Useful
flags include:

* ``--circuit <name>`` – run a single circuit (repeat the flag to add more).
* ``--group <name>`` – run every circuit in the named group.
* ``--list-circuits`` / ``--list-groups`` – inspect available names and exit.
* ``--workers <n>`` – limit the number of worker threads (defaults to the
  auto-detected concurrency used by :mod:`benchmarks.bench_utils`).
* ``--qubits name=4:10:2`` – override the qubit widths for a specific circuit.
* ``--enable-classical-simplification`` – enable classical-control
  simplification for all generated circuits.
* ``--estimate`` – generate theoretical estimates after the showcase run.

When no circuits or groups are specified the full suite is executed.  Reuse the
``--reuse-existing`` flag to skip recomputation if the CSV files already exist.

### Programmatic access

The CLI delegates to :func:`benchmarks.run_benchmark.run_showcase_suite` for
automation and testing.  The helper returns a :class:`pandas.DataFrame` with the
same structure as the persisted CSV output:

```python
from benchmarks.run_benchmark import run_showcase_suite

df = run_showcase_suite(
    "classical_controlled",
    widths=(2,),
    repetitions=1,
    workers=1,
)
```

## Theoretical estimation

Add the ``--estimate`` flag to ``run_benchmark.py`` to generate analytical
runtime and memory predictions after executing the benchmarks.  Use
``--estimate-only`` to skip the simulator runs entirely.  The theoretical
estimates reuse the cost-estimator helpers in
``benchmarks/bench_utils/theoretical_estimation_utils.py`` and produce CSV
tables and figures in ``benchmarks/bench_utils/results``.

Advanced flags:

* ``--ops-per-second`` – set the conversion factor from model operations to
  seconds (use ``0`` to omit runtime conversion).
* ``--calibration`` – provide a JSON file with calibrated cost coefficients.

Programmatic access is exposed via
:func:`benchmarks.run_benchmark.generate_theoretical_estimates`, which returns
both the detailed and summary :class:`pandas.DataFrame` objects.

## Quick inspection

For ad-hoc inspection of existing results launch the notebooks in
``benchmarks/notebooks``:

```bash
jupyter notebook benchmarks/notebooks/comparison.ipynb
```

The notebooks expect the CSV artefacts generated by the showcase suite.

