{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaf2441",
   "metadata": {},
   "source": [
    "# Partitioning trade-offs in QuASAr\n",
    "\n",
    "This notebook explores how QuASAr's cost estimator responds to different partitioning strategies.  Synthetic fragments are used to probe backend feasibility, runtime and memory projections, and the conversion primitives required to glue heterogeneous plans together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4f250",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "* Provide an editable sandbox for fragment parameters (size, sparsity, locality, resource ceilings) and visualise backend feasibility.\n",
    "* Compare monolithic execution against two- and three-segment plans that incorporate conversion costs, highlighting when partitioning improves the projected runtime or memory footprint.\n",
    "* Map how gate mix, boundary widths and conversion primitives interact to make partitioning advantageous according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b32cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Sequence\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import display\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"pyproject.toml\").exists():\n",
    "    for candidate in PROJECT_ROOT.parents:\n",
    "        if (candidate / \"pyproject.toml\").exists():\n",
    "            PROJECT_ROOT = candidate\n",
    "            break\n",
    "\n",
    "import sys\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from docs.utils.partitioning_analysis import (\n",
    "    FragmentStats,\n",
    "    BoundarySpec,\n",
    "    evaluate_fragment_backends,\n",
    "    aggregate_partitioned_plan,\n",
    "    aggregate_single_backend_plan,\n",
    ")\n",
    "from quasar.cost import Backend, CostEstimator\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "pd.set_option(\"display.max_columns\", 0)\n",
    "pd.set_option(\"display.float_format\", lambda v: f\"{v:,.3g}\")\n",
    "\n",
    "ESTIMATOR = CostEstimator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesise_fragment(\n",
    "    num_qubits: int,\n",
    "    depth: int,\n",
    "    entangling_ratio: float,\n",
    "    measurement_ratio: float = 0.0,\n",
    "    *,\n",
    "    is_clifford: bool = False,\n",
    "    is_local: bool = False,\n",
    "    frontier: int | None = None,\n",
    "    frontier_scale: float | None = None,\n",
    "    chi: int | Sequence[int] | None = None,\n",
    ") -> FragmentStats:\n",
    "    \"\"\"Create a FragmentStats instance using coarse circuit descriptors.\"\"\"\n",
    "\n",
    "    depth = max(int(depth), 1)\n",
    "    entangling_ratio = float(entangling_ratio)\n",
    "    entangling_layers = max(0, min(depth, int(round(depth * entangling_ratio))))\n",
    "    one_qubit_layers = depth - entangling_layers\n",
    "    entangling_pairs = max(num_qubits - 1, 1)\n",
    "    num_2q_gates = entangling_layers * entangling_pairs\n",
    "    num_1q_gates = one_qubit_layers * num_qubits\n",
    "    num_measurements = int(round(measurement_ratio * num_qubits))\n",
    "    if frontier is None and frontier_scale is not None:\n",
    "        frontier = max(1, int(round(num_qubits * frontier_scale)))\n",
    "    return FragmentStats(\n",
    "        num_qubits=num_qubits,\n",
    "        num_1q_gates=num_1q_gates,\n",
    "        num_2q_gates=num_2q_gates,\n",
    "        num_measurements=num_measurements,\n",
    "        is_clifford=is_clifford,\n",
    "        is_local=is_local,\n",
    "        frontier=frontier,\n",
    "        chi=chi,\n",
    "    )\n",
    "\n",
    "\n",
    "def _to_iterable(value) -> list:\n",
    "    if isinstance(value, (list, tuple, set, range)):\n",
    "        return list(value)\n",
    "    if hasattr(value, \"tolist\"):\n",
    "        return list(value)\n",
    "    return [value]\n",
    "\n",
    "\n",
    "def _limit_to_bytes(value: float | None) -> float | None:\n",
    "    if value is None:\n",
    "        return None\n",
    "    return float(value) * (1024**3)\n",
    "\n",
    "\n",
    "def safe_log10(values: Iterable[float], *, floor: float = -12.0) -> np.ndarray:\n",
    "    array = np.asarray(list(values), dtype=float)\n",
    "    finite = np.maximum(array, 10 ** floor)\n",
    "    return np.log10(finite)\n",
    "\n",
    "\n",
    "def evaluate_parameter_grid(\n",
    "    fragment_axes: dict,\n",
    "    metric_axes: dict,\n",
    "    resource_limits: dict,\n",
    "    *,\n",
    "    allow_tableau: bool = True,\n",
    "    estimator: CostEstimator | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Sweep synthetic fragment parameters and record backend selections.\"\"\"\n",
    "\n",
    "    estimator = estimator or CostEstimator()\n",
    "    max_memory = _limit_to_bytes(resource_limits.get(\"max_memory_gb\"))\n",
    "    max_time = resource_limits.get(\"max_time_s\")\n",
    "\n",
    "    fragment_keys = list(fragment_axes.keys())\n",
    "    metric_keys = list(metric_axes.keys())\n",
    "    metric_products = [_to_iterable(metric_axes[key]) for key in metric_keys]\n",
    "    if not metric_products:\n",
    "        metric_products = [[]]\n",
    "\n",
    "    rows: list[dict] = []\n",
    "    for frag_values in itertools.product(*(_to_iterable(fragment_axes[key]) for key in fragment_keys)):\n",
    "        frag_params = dict(zip(fragment_keys, frag_values))\n",
    "        stats = synthesise_fragment(**frag_params)\n",
    "        for metric_values in itertools.product(*metric_products):\n",
    "            metrics = dict(zip(metric_keys, metric_values))\n",
    "            backend, diag = evaluate_fragment_backends(\n",
    "                stats,\n",
    "                sparsity=metrics.get(\"sparsity\"),\n",
    "                phase_rotation_diversity=metrics.get(\"phase_rotation_diversity\"),\n",
    "                amplitude_rotation_diversity=metrics.get(\"amplitude_rotation_diversity\"),\n",
    "                allow_tableau=allow_tableau,\n",
    "                max_memory=max_memory,\n",
    "                max_time=max_time,\n",
    "                estimator=estimator,\n",
    "            )\n",
    "            if backend is None:\n",
    "                selected_time = math.nan\n",
    "                selected_memory = math.nan\n",
    "            else:\n",
    "                selected_time = diag[\"selected_cost\"].time\n",
    "                selected_memory = diag[\"selected_cost\"].memory\n",
    "            row = {**frag_params, **metrics}\n",
    "            row[\"selected_backend\"] = backend.name if backend else None\n",
    "            row[\"selected_time\"] = selected_time\n",
    "            row[\"selected_memory\"] = selected_memory\n",
    "            for cand_backend, entry in diag[\"backends\"].items():\n",
    "                label = cand_backend.name.lower()\n",
    "                feasible = entry.get(\"feasible\") if isinstance(entry, dict) else None\n",
    "                cost = entry.get(\"cost\") if isinstance(entry, dict) else None\n",
    "                row[f\"{label}_feasible\"] = feasible\n",
    "                row[f\"{label}_time\"] = cost.time if cost else math.nan\n",
    "                row[f\"{label}_memory\"] = cost.memory if cost else math.nan\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_plan(\n",
    "    fragment_stats: Sequence[FragmentStats],\n",
    "    fragment_metrics: Sequence[dict],\n",
    "    *,\n",
    "    boundaries: Sequence[BoundarySpec] | None = None,\n",
    "    resource_limits: dict | None = None,\n",
    "    allow_tableau: bool = True,\n",
    "    estimator: CostEstimator | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"Choose backends for each fragment and aggregate plan costs.\"\"\"\n",
    "\n",
    "    if len(fragment_stats) != len(fragment_metrics):\n",
    "        raise ValueError(\"metrics must align with fragment list\")\n",
    "    estimator = estimator or CostEstimator()\n",
    "    resource_limits = resource_limits or {}\n",
    "    max_memory = _limit_to_bytes(resource_limits.get(\"max_memory_gb\"))\n",
    "    max_time = resource_limits.get(\"max_time_s\")\n",
    "\n",
    "    selections: list[tuple[Backend, object]] = []\n",
    "    diagnostics: list[dict] = []\n",
    "    for stats, metrics in zip(fragment_stats, fragment_metrics):\n",
    "        backend, diag = evaluate_fragment_backends(\n",
    "            stats,\n",
    "            sparsity=metrics.get(\"sparsity\"),\n",
    "            phase_rotation_diversity=metrics.get(\"phase_rotation_diversity\"),\n",
    "            amplitude_rotation_diversity=metrics.get(\"amplitude_rotation_diversity\"),\n",
    "            allow_tableau=allow_tableau,\n",
    "            max_memory=max_memory,\n",
    "            max_time=max_time,\n",
    "            estimator=estimator,\n",
    "        )\n",
    "        if backend is None:\n",
    "            raise RuntimeError(\"fragment infeasible under the selected limits\")\n",
    "        selections.append((backend, diag[\"selected_cost\"]))\n",
    "        diagnostics.append(diag)\n",
    "\n",
    "    if boundaries:\n",
    "        plan = aggregate_partitioned_plan(selections, boundaries, estimator=estimator)\n",
    "        total_cost = plan[\"total_cost\"]\n",
    "        conversions = plan[\"conversions\"]\n",
    "    else:\n",
    "        total_cost = aggregate_single_backend_plan(selections)\n",
    "        conversions = []\n",
    "    return {\n",
    "        \"fragments\": diagnostics,\n",
    "        \"selections\": selections,\n",
    "        \"total_cost\": total_cost,\n",
    "        \"conversions\": conversions,\n",
    "    }\n",
    "\n",
    "\n",
    "def plan_overview(label: str, plan: dict) -> pd.DataFrame:\n",
    "    cost = plan[\"total_cost\"]\n",
    "    backends = \" → \".join(selection[0].name for selection in plan[\"selections\"])\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"plan\": label,\n",
    "                \"backends\": backends,\n",
    "                \"total_time\": cost.time,\n",
    "                \"peak_memory\": cost.memory,\n",
    "                \"conversion_time\": cost.conversion,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def fragment_breakdown(plan: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for idx, (backend, cost) in enumerate(plan[\"selections\"]):\n",
    "        diag = plan[\"fragments\"][idx]\n",
    "        metrics = diag.get(\"metrics\", {})\n",
    "        rows.append(\n",
    "            {\n",
    "                \"fragment\": idx,\n",
    "                \"backend\": backend.name,\n",
    "                \"num_qubits\": metrics.get(\"num_qubits\"),\n",
    "                \"num_gates\": metrics.get(\"num_gates\"),\n",
    "                \"sparsity\": metrics.get(\"sparsity\"),\n",
    "                \"time\": cost.time,\n",
    "                \"memory\": cost.memory,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def conversion_breakdown(plan: dict) -> pd.DataFrame:\n",
    "    if not plan[\"conversions\"]:\n",
    "        return pd.DataFrame(columns=[\"index\", \"source\", \"target\", \"primitive\", \"time\", \"memory\"])\n",
    "    rows = []\n",
    "    for entry in plan[\"conversions\"]:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"index\": entry[\"index\"],\n",
    "                \"source\": entry[\"source\"].name,\n",
    "                \"target\": entry[\"target\"].name,\n",
    "                \"primitive\": entry[\"primitive\"],\n",
    "                \"time\": entry[\"cost\"].time,\n",
    "                \"memory\": entry[\"cost\"].memory,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def evaluate_partition_advantage(\n",
    "    gate_mixes: Sequence[float],\n",
    "    boundary_qubits: Sequence[int],\n",
    "    ranks: Sequence[int],\n",
    "    *,\n",
    "    total_qubits: int = 34,\n",
    "    total_depth: int = 72,\n",
    "    local_threshold: float = 0.32,\n",
    "    estimator: CostEstimator | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Map when a two-fragment plan beats a monolithic execution.\"\"\"\n",
    "\n",
    "    estimator = estimator or CostEstimator()\n",
    "    rows: list[dict] = []\n",
    "    depth_a = int(round(total_depth * 0.55))\n",
    "    depth_b = total_depth - depth_a\n",
    "\n",
    "    for gate_mix, boundary, rank in itertools.product(gate_mixes, boundary_qubits, ranks):\n",
    "        monolithic = synthesise_fragment(total_qubits, total_depth, gate_mix, is_local=False)\n",
    "        mono_backend, mono_diag = evaluate_fragment_backends(\n",
    "            monolithic,\n",
    "            sparsity=max(0.35, 1.0 - gate_mix * 1.4),\n",
    "            estimator=estimator,\n",
    "        )\n",
    "        mono_cost = mono_diag[\"selected_cost\"]\n",
    "\n",
    "        local_flag = gate_mix <= local_threshold\n",
    "        if local_flag:\n",
    "            frag_a = synthesise_fragment(\n",
    "                18,\n",
    "                depth_a,\n",
    "                max(0.08, gate_mix * 0.55),\n",
    "                is_local=True,\n",
    "                frontier_scale=0.24,\n",
    "                chi=64,\n",
    "            )\n",
    "            sparsity_a = min(0.95, 0.8 + (0.26 - gate_mix) * 1.5)\n",
    "        else:\n",
    "            frag_a = synthesise_fragment(\n",
    "                total_qubits,\n",
    "                depth_a,\n",
    "                gate_mix * 0.9,\n",
    "                is_local=False,\n",
    "            )\n",
    "            sparsity_a = max(0.35, 1.0 - gate_mix * 1.1)\n",
    "\n",
    "        frag_b = synthesise_fragment(total_qubits, depth_b, gate_mix, is_local=False)\n",
    "        sparsity_b = max(0.35, 1.0 - gate_mix * 1.3)\n",
    "\n",
    "        sel_a, diag_a = evaluate_fragment_backends(\n",
    "            frag_a,\n",
    "            sparsity=sparsity_a,\n",
    "            estimator=estimator,\n",
    "        )\n",
    "        sel_b, diag_b = evaluate_fragment_backends(\n",
    "            frag_b,\n",
    "            sparsity=sparsity_b,\n",
    "            estimator=estimator,\n",
    "        )\n",
    "\n",
    "        boundary_spec = BoundarySpec(\n",
    "            num_qubits=boundary,\n",
    "            rank=rank,\n",
    "            frontier=max(boundary, 12),\n",
    "            window=min(10, boundary // 2 + 2),\n",
    "            window_1q_gates=boundary * 4,\n",
    "            window_2q_gates=boundary * 2,\n",
    "        )\n",
    "\n",
    "        plan = aggregate_partitioned_plan(\n",
    "            [(sel_a, diag_a[\"selected_cost\"]), (sel_b, diag_b[\"selected_cost\"])],\n",
    "            [boundary_spec],\n",
    "            estimator=estimator,\n",
    "        )\n",
    "        total = plan[\"total_cost\"]\n",
    "        primitive = plan[\"conversions\"][0][\"primitive\"] if plan[\"conversions\"] else \"None\"\n",
    "        rows.append(\n",
    "            {\n",
    "                \"gate_mix\": gate_mix,\n",
    "                \"boundary_qubits\": boundary,\n",
    "                \"rank\": rank,\n",
    "                \"fragment_a_backend\": sel_a.name,\n",
    "                \"fragment_b_backend\": sel_b.name,\n",
    "                \"monolithic_backend\": mono_backend.name if mono_backend else None,\n",
    "                \"partition_time\": total.time,\n",
    "                \"monolithic_time\": mono_cost.time,\n",
    "                \"speedup\": mono_cost.time / total.time if total.time else float(\"inf\"),\n",
    "                \"primitive\": primitive,\n",
    "                \"partition_wins\": total.time < mono_cost.time,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af478d0",
   "metadata": {},
   "source": [
    "## Parameter grid (edit me)\n",
    "\n",
    "The dictionaries below define the parameter sweep for the feasibility study.  Update the lists to explore different fragment sizes, sparsities or resource ceilings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbeaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_axes = {\n",
    "    \"num_qubits\": [18, 24, 30],\n",
    "    \"depth\": [48, 64],\n",
    "    \"entangling_ratio\": [0.18, 0.28, 0.38],\n",
    "    \"measurement_ratio\": [0.0],\n",
    "    \"is_clifford\": [False],\n",
    "    \"is_local\": [False, True],\n",
    "    \"frontier\": [None],\n",
    "    \"frontier_scale\": [0.25],\n",
    "    \"chi\": [None],\n",
    "}\n",
    "metric_axes = {\n",
    "    \"sparsity\": np.linspace(0.55, 0.9, 4),\n",
    "    \"phase_rotation_diversity\": [6],\n",
    "    \"amplitude_rotation_diversity\": [8],\n",
    "}\n",
    "resource_limits = {\"max_memory_gb\": 64, \"max_time_s\": None}\n",
    "ALLOW_TABLEAU = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72eedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = evaluate_parameter_grid(\n",
    "    fragment_axes,\n",
    "    metric_axes,\n",
    "    resource_limits,\n",
    "    allow_tableau=ALLOW_TABLEAU,\n",
    "    estimator=ESTIMATOR,\n",
    ")\n",
    "grid_results[\"log_selected_time\"] = safe_log10(grid_results[\"selected_time\"])\n",
    "grid_results[\"log_selected_memory\"] = safe_log10(grid_results[\"selected_memory\"])\n",
    "grid_results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b181cb",
   "metadata": {},
   "source": [
    "### Backend feasibility maps\n",
    "\n",
    "The heatmaps below show the preferred backend across fragment sizes and sparsities, separated by the locality flag.  The accompanying plot visualises the log10 runtime estimate for the selected backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c11010",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_order = [\"None\", \"STATEVECTOR\", \"DECISION_DIAGRAM\", \"MPS\", \"TABLEAU\"]\n",
    "backend_to_idx = {name: idx for idx, name in enumerate(backend_order)}\n",
    "colors = [\"#dcdcdc\", \"#4477aa\", \"#66c2a5\", \"#ffa600\", \"#aa3377\"]\n",
    "backend_cmap = ListedColormap(colors)\n",
    "\n",
    "selected_depth = _to_iterable(fragment_axes[\"depth\"])[0]\n",
    "subset = grid_results[grid_results[\"depth\"] == selected_depth]\n",
    "if \"phase_rotation_diversity\" in subset and not subset[\"phase_rotation_diversity\"].isna().all():\n",
    "    subset = subset[subset[\"phase_rotation_diversity\"] == _to_iterable(metric_axes[\"phase_rotation_diversity\"])[0]]\n",
    "if \"amplitude_rotation_diversity\" in subset and not subset[\"amplitude_rotation_diversity\"].isna().all():\n",
    "    subset = subset[subset[\"amplitude_rotation_diversity\"] == _to_iterable(metric_axes[\"amplitude_rotation_diversity\"])[0]]\n",
    "\n",
    "def _heatmap_payload(frame: pd.DataFrame):\n",
    "    qubits = sorted(frame[\"num_qubits\"].unique())\n",
    "    sparsities = sorted(frame[\"sparsity\"].unique())\n",
    "    data = np.full((len(sparsities), len(qubits)), np.nan)\n",
    "    log_time = np.full_like(data, np.nan)\n",
    "    for i, sparsity in enumerate(sparsities):\n",
    "        for j, nq in enumerate(qubits):\n",
    "            sample = frame[(frame[\"num_qubits\"] == nq) & (frame[\"sparsity\"] == sparsity)]\n",
    "            if sample.empty:\n",
    "                continue\n",
    "            backend = sample.iloc[0][\"selected_backend\"] or \"None\"\n",
    "            data[i, j] = backend_to_idx.get(backend, np.nan)\n",
    "            log_time[i, j] = sample.iloc[0][\"log_selected_time\"]\n",
    "    return qubits, sparsities, data, log_time\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "for ax_idx, is_local in enumerate([False, True]):\n",
    "    frame = subset[subset[\"is_local\"] == is_local]\n",
    "    qubits, sparsities, data, log_time = _heatmap_payload(frame)\n",
    "    axes[0, ax_idx].imshow(data, cmap=backend_cmap, aspect=\"auto\", origin=\"lower\", vmin=0, vmax=len(backend_order) - 1)\n",
    "    axes[0, ax_idx].set_title(f\"is_local = {is_local}\")\n",
    "    axes[0, ax_idx].set_xticks(range(len(qubits)), qubits)\n",
    "    axes[0, ax_idx].set_yticks(range(len(sparsities)), [f\"{val:.2f}\" for val in sparsities])\n",
    "    axes[0, ax_idx].set_ylabel(\"sparsity\")\n",
    "    axes[1, ax_idx].imshow(log_time, cmap=\"viridis\", aspect=\"auto\", origin=\"lower\")\n",
    "    axes[1, ax_idx].set_xticks(range(len(qubits)), qubits)\n",
    "    axes[1, ax_idx].set_yticks(range(len(sparsities)), [f\"{val:.2f}\" for val in sparsities])\n",
    "    axes[1, ax_idx].set_xlabel(\"fragment qubits\")\n",
    "    axes[1, ax_idx].set_ylabel(\"sparsity\")\n",
    "fig.colorbar(plt.cm.ScalarMappable(cmap=backend_cmap), ax=axes[0, :], ticks=range(len(backend_order)), orientation=\"horizontal\", pad=0.1, label=\"selected backend\")\n",
    "fig.colorbar(plt.cm.ScalarMappable(cmap=\"viridis\"), ax=axes[1, :], orientation=\"horizontal\", pad=0.1, label=\"log10 runtime estimate\")\n",
    "plt.suptitle(\"Backend selection and runtime across the parameter sweep\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47acd5",
   "metadata": {},
   "source": [
    "### Case study: two segments with a decision diagram prefix\n",
    "\n",
    "A sparse, locally entangling prefix can often be simulated efficiently with the decision-diagram backend before converting into a dense statevector section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_case1 = {\"max_memory_gb\": 64}\n",
    "monolithic_stats = synthesise_fragment(34, 70, 0.35, is_local=False)\n",
    "monolithic_plan = run_plan([monolithic_stats], [{\"sparsity\": 0.62}], resource_limits=resource_case1, estimator=ESTIMATOR)\n",
    "\n",
    "fragment_a = synthesise_fragment(18, 44, 0.18, is_local=True, frontier_scale=0.25, chi=48)\n",
    "fragment_b = synthesise_fragment(34, 32, 0.32, is_local=False)\n",
    "boundary_ab = BoundarySpec(num_qubits=14, rank=48, frontier=20, window=8, window_1q_gates=60, window_2q_gates=16)\n",
    "partition_plan = run_plan(\n",
    "    [fragment_a, fragment_b],\n",
    "    [{\"sparsity\": 0.88}, {\"sparsity\": 0.58}],\n",
    "    boundaries=[boundary_ab],\n",
    "    resource_limits=resource_case1,\n",
    "    estimator=ESTIMATOR,\n",
    ")\n",
    "\n",
    "overview = pd.concat(\n",
    "    [\n",
    "        plan_overview(\"Monolithic statevector\", monolithic_plan),\n",
    "        plan_overview(\"DD → statevector\", partition_plan),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "overview[\"time_speedup_vs_monolithic\"] = overview.loc[0, \"total_time\"] / overview[\"total_time\"]\n",
    "overview[\"memory_ratio_vs_monolithic\"] = overview.loc[0, \"peak_memory\"] / overview[\"peak_memory\"]\n",
    "display(overview)\n",
    "\n",
    "display(fragment_breakdown(partition_plan))\n",
    "conversion_df = conversion_breakdown(partition_plan)\n",
    "if not conversion_df.empty:\n",
    "    display(conversion_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axes[0].bar(overview[\"plan\"], safe_log10(overview[\"total_time\"]))\n",
    "axes[0].set_ylabel(\"log10 total time\")\n",
    "axes[1].bar(overview[\"plan\"], safe_log10(overview[\"peak_memory\"]))\n",
    "axes[1].set_ylabel(\"log10 peak memory\")\n",
    "plt.suptitle(\"Two-segment plan compared to the monolithic execution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6e16b",
   "metadata": {},
   "source": [
    "### Case study: three segments with conversions\n",
    "\n",
    "A Clifford initialisation, a local MPS window and a dense finale illustrate how multiple conversions accumulate while still reducing projected runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_case2 = {\"max_memory_gb\": 128}\n",
    "mono_stats = synthesise_fragment(48, 90, 0.32, is_local=False)\n",
    "mono_plan = run_plan([mono_stats], [{\"sparsity\": 0.58}], resource_limits=resource_case2, estimator=ESTIMATOR)\n",
    "\n",
    "frag1 = synthesise_fragment(48, 18, 0.0, is_clifford=True, is_local=False)\n",
    "frag2 = synthesise_fragment(32, 50, 0.18, is_local=True, frontier_scale=0.2, chi=48)\n",
    "frag3 = synthesise_fragment(48, 36, 0.34, is_local=False)\n",
    "\n",
    "boundary_12 = BoundarySpec(num_qubits=12, rank=32, frontier=24, window=6, window_1q_gates=40, window_2q_gates=12)\n",
    "boundary_23 = BoundarySpec(num_qubits=18, rank=64, frontier=30, window=8, window_1q_gates=60, window_2q_gates=20)\n",
    "three_plan = run_plan(\n",
    "    [frag1, frag2, frag3],\n",
    "    [{\"sparsity\": 0.95}, {\"sparsity\": 0.88}, {\"sparsity\": 0.55}],\n",
    "    boundaries=[boundary_12, boundary_23],\n",
    "    resource_limits=resource_case2,\n",
    "    estimator=ESTIMATOR,\n",
    ")\n",
    "\n",
    "overview_three = pd.concat(\n",
    "    [\n",
    "        plan_overview(\"Monolithic statevector\", mono_plan),\n",
    "        plan_overview(\"Tableau → MPS → statevector\", three_plan),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "overview_three[\"time_speedup_vs_monolithic\"] = overview_three.loc[0, \"total_time\"] / overview_three[\"total_time\"]\n",
    "overview_three[\"memory_ratio_vs_monolithic\"] = overview_three.loc[0, \"peak_memory\"] / overview_three[\"peak_memory\"]\n",
    "display(overview_three)\n",
    "\n",
    "display(fragment_breakdown(three_plan))\n",
    "conv_three = conversion_breakdown(three_plan)\n",
    "if not conv_three.empty:\n",
    "    display(conv_three)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axes[0].bar(overview_three[\"plan\"], safe_log10(overview_three[\"total_time\"]))\n",
    "axes[0].set_ylabel(\"log10 total time\")\n",
    "axes[1].bar(overview_three[\"plan\"], safe_log10(overview_three[\"peak_memory\"]))\n",
    "axes[1].set_ylabel(\"log10 peak memory\")\n",
    "plt.suptitle(\"Three-segment plan compared to the monolithic execution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e0fd8",
   "metadata": {},
   "source": [
    "### Feature map for partition advantage\n",
    "\n",
    "The following sweep varies the gate mix (fraction of entangling layers), conversion boundary width and Schmidt-rank cap.  It highlights where the model predicts a win for the two-fragment plan and which conversion primitive is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_df = evaluate_partition_advantage(\n",
    "    gate_mixes=[0.22, 0.28, 0.34, 0.4],\n",
    "    boundary_qubits=[8, 12, 16, 20, 24, 28],\n",
    "    ranks=[16, 32, 64],\n",
    "    estimator=ESTIMATOR,\n",
    ")\n",
    "partition_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200682d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage = partition_df[partition_df[\"partition_wins\"]].copy()\n",
    "losses = partition_df[~partition_df[\"partition_wins\"]].copy()\n",
    "\n",
    "summary = (\n",
    "    advantage.groupby([\"gate_mix\", \"primitive\"])\n",
    "    .agg(\n",
    "        min_boundary=(\"boundary_qubits\", \"min\"),\n",
    "        max_speedup=(\"speedup\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"gate_mix\", \"min_boundary\"])\n",
    ")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not losses.empty:\n",
    "    loss_summary = (\n",
    "        losses.groupby([\"gate_mix\", \"primitive\"])\n",
    "        .agg(\n",
    "            min_boundary=(\"boundary_qubits\", \"min\"),\n",
    "            max_boundary=(\"boundary_qubits\", \"max\"),\n",
    "            worst_speedup=(\"speedup\", \"min\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"gate_mix\", \"min_boundary\"])\n",
    "    )\n",
    "    display(loss_summary)\n",
    "else:\n",
    "    print(\"Partitioning won for every sampled configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e74681",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(sorted(partition_df[\"rank\"].unique())), figsize=(15, 3), sharey=True)\n",
    "for ax, rank in zip(np.atleast_1d(axes), sorted(partition_df[\"rank\"].unique())):\n",
    "    view = partition_df[partition_df[\"rank\"] == rank]\n",
    "    pivot = view.pivot_table(index=\"gate_mix\", columns=\"boundary_qubits\", values=\"speedup\", aggfunc=\"mean\")\n",
    "    im = ax.imshow(np.log10(pivot.values), cmap=\"viridis\", aspect=\"auto\", origin=\"lower\")\n",
    "    ax.set_title(f\"rank ≤ {rank}\")\n",
    "    ax.set_xticks(range(len(pivot.columns)), pivot.columns)\n",
    "    ax.set_yticks(range(len(pivot.index)), [f\"{v:.2f}\" for v in pivot.index])\n",
    "    ax.set_xlabel(\"boundary qubits\")\n",
    "axes[0].set_ylabel(\"gate mix (entangling fraction)\")\n",
    "fig.colorbar(im, ax=axes, orientation=\"horizontal\", fraction=0.04, pad=0.1, label=\"log10 speedup\")\n",
    "plt.suptitle(\"Speedup heatmap across boundary sizes and gate mixes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0db13",
   "metadata": {},
   "source": [
    "The remaining tables characterise losing configurations and conversion choices.  When the initial fragment is forced onto the dense statevector backend, the conversions disappear and the partition offers no benefit, underscoring the importance of sparsity or locality for heterogeneous plans."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
