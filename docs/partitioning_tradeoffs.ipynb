{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaf2441",
   "metadata": {},
   "source": [
    "# Partitioning trade-offs in QuASAr\n",
    "\n",
    "This notebook explores how QuASAr's cost estimator responds to different partitioning strategies.  Synthetic fragments are used to probe backend feasibility, runtime and memory projections, and the conversion primitives required to glue heterogeneous plans together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4f250",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "* Provide an editable sandbox for fragment parameters (size, sparsity, locality, resource ceilings) and visualise backend feasibility.\n",
    "* Compare monolithic execution against two- and three-segment plans that incorporate conversion costs, highlighting when partitioning improves the projected runtime or memory footprint.\n",
    "* Map how gate mix, boundary widths and conversion primitives interact to make partitioning advantageous according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f86ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "from collections.abc import Iterable, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from docs.utils.partitioning_analysis import (\n",
    "    FragmentStats,\n",
    "    BoundarySpec,\n",
    "    aggregate_partitioned_plan,\n",
    "    evaluate_fragment_backends,\n",
    ")\n",
    "from quasar.cost import Backend, Cost, CostEstimator\n",
    "\n",
    "def synthesise_fragment(\n",
    "    num_qubits: int,\n",
    "    depth: int,\n",
    "    entangling_ratio: float,\n",
    "    measurement_ratio: float = 0.0,\n",
    "    *,\n",
    "    is_clifford: bool = False,\n",
    "    is_local: bool = False,\n",
    "    frontier: int | None = None,\n",
    "    frontier_scale: float | None = None,\n",
    "    chi: int | Sequence[int] | None = None,\n",
    ") -> FragmentStats:\n",
    "    \"\"\"Create a FragmentStats instance using coarse circuit descriptors.\"\"\"\n",
    "\n",
    "    depth = max(int(depth), 1)\n",
    "    entangling_ratio = float(entangling_ratio)\n",
    "    entangling_layers = max(0, min(depth, int(round(depth * entangling_ratio))))\n",
    "    one_qubit_layers = depth - entangling_layers\n",
    "    entangling_pairs = max(num_qubits - 1, 1)\n",
    "    num_2q_gates = entangling_layers * entangling_pairs\n",
    "    num_1q_gates = one_qubit_layers * num_qubits\n",
    "    num_measurements = int(round(measurement_ratio * num_qubits))\n",
    "    if frontier is None and frontier_scale is not None:\n",
    "        frontier = max(1, int(round(num_qubits * frontier_scale)))\n",
    "    return FragmentStats(\n",
    "        num_qubits=num_qubits,\n",
    "        num_1q_gates=num_1q_gates,\n",
    "        num_2q_gates=num_2q_gates,\n",
    "        num_measurements=num_measurements,\n",
    "        is_clifford=is_clifford,\n",
    "        is_local=is_local,\n",
    "        frontier=frontier,\n",
    "        chi=chi,\n",
    "    )\n",
    "\n",
    "\n",
    "def _to_iterable(value) -> list:\n",
    "    if isinstance(value, (list, tuple, set, range)):\n",
    "        return list(value)\n",
    "    if hasattr(value, \"tolist\"):\n",
    "        return list(value)\n",
    "    return [value]\n",
    "\n",
    "\n",
    "def _limit_to_bytes(value: float | None) -> float | None:\n",
    "    if value is None:\n",
    "        return None\n",
    "    return float(value) * (1024**3)\n",
    "\n",
    "\n",
    "def safe_log10(values: Iterable[float], *, floor: float = -12.0) -> np.ndarray:\n",
    "    array = np.asarray(list(values), dtype=float)\n",
    "    finite = np.maximum(array, 10 ** floor)\n",
    "    return np.log10(finite)\n",
    "\n",
    "\n",
    "def evaluate_parameter_grid(\n",
    "    fragment_axes: dict,\n",
    "    metric_axes: dict,\n",
    "    resource_limits: dict,\n",
    "    *,\n",
    "    allow_tableau: bool = True,\n",
    "    estimator: CostEstimator | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Sweep synthetic fragment parameters and record backend selections.\"\"\"\n",
    "\n",
    "    estimator = estimator or CostEstimator()\n",
    "    max_memory = _limit_to_bytes(resource_limits.get(\"max_memory_gb\"))\n",
    "    max_time = resource_limits.get(\"max_time_s\")\n",
    "\n",
    "    fragment_keys = list(fragment_axes.keys())\n",
    "    metric_keys = list(metric_axes.keys())\n",
    "    metric_products = [_to_iterable(metric_axes[key]) for key in metric_keys]\n",
    "    if not metric_products:\n",
    "        metric_products = [[]]\n",
    "\n",
    "    rows: list[dict] = []\n",
    "    for frag_values in itertools.product(*(_to_iterable(fragment_axes[key]) for key in fragment_keys)):\n",
    "        frag_params = dict(zip(fragment_keys, frag_values))\n",
    "        stats = synthesise_fragment(**frag_params)\n",
    "        for metric_values in itertools.product(*metric_products):\n",
    "            metrics = dict(zip(metric_keys, metric_values))\n",
    "            backend, diag = evaluate_fragment_backends(\n",
    "                stats,\n",
    "                sparsity=metrics.get(\"sparsity\"),\n",
    "                phase_rotation_diversity=metrics.get(\"phase_rotation_diversity\"),\n",
    "                amplitude_rotation_diversity=metrics.get(\"amplitude_rotation_diversity\"),\n",
    "                allow_tableau=allow_tableau,\n",
    "                max_memory=max_memory,\n",
    "                max_time=max_time,\n",
    "                estimator=estimator,\n",
    "            )\n",
    "            if backend is None:\n",
    "                selected_time = math.nan\n",
    "                selected_memory = math.nan\n",
    "            else:\n",
    "                selected_time = diag[\"selected_cost\"].time\n",
    "                selected_memory = diag[\"selected_cost\"].memory\n",
    "            row = {**frag_params, **metrics}\n",
    "            row[\"num_1q_gates\"] = stats.num_1q_gates\n",
    "            row[\"num_2q_gates\"] = stats.num_2q_gates\n",
    "            row[\"num_measurements\"] = stats.num_measurements\n",
    "            row[\"selected_backend\"] = backend.name if backend else None\n",
    "            row[\"selected_time\"] = selected_time\n",
    "            row[\"selected_memory\"] = selected_memory\n",
    "            for cand_backend, entry in diag[\"backends\"].items():\n",
    "                label = cand_backend.name.lower()\n",
    "                feasible = entry.get(\"feasible\") if isinstance(entry, dict) else None\n",
    "                cost = entry.get(\"cost\") if isinstance(entry, dict) else None\n",
    "                row[f\"{label}_feasible\"] = feasible\n",
    "                row[f\"{label}_time\"] = cost.time if cost else math.nan\n",
    "                row[f\"{label}_memory\"] = cost.memory if cost else math.nan\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def run_plan(\n",
    "    fragment_stats: Sequence[FragmentStats],\n",
    "    fragment_metrics: Sequence[dict],\n",
    "    *,\n",
    "    boundaries: Sequence[BoundarySpec] | None = None,\n",
    "    resource_limits: dict | None = None,\n",
    "    allow_tableau: bool = True,\n",
    "    estimator: CostEstimator | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"Choose backends for each fragment and aggregate plan costs.\"\"\"\n",
    "\n",
    "    if len(fragment_stats) != len(fragment_metrics):\n",
    "        raise ValueError(\"metrics must align with fragment list\")\n",
    "    estimator = estimator or CostEstimator()\n",
    "    resource_limits = resource_limits or {}\n",
    "    max_memory = _limit_to_bytes(resource_limits.get(\"max_memory_gb\"))\n",
    "    max_time = resource_limits.get(\"max_time_s\")\n",
    "\n",
    "    selections: list[tuple[Backend, object]] = []\n",
    "    diagnostics: list[dict] = []\n",
    "    for stats, metrics in zip(fragment_stats, fragment_metrics):\n",
    "        backend, diag = evaluate_fragment_backends(\n",
    "            stats,\n",
    "            sparsity=metrics.get(\"sparsity\"),\n",
    "            phase_rotation_diversity=metrics.get(\"phase_rotation_diversity\"),\n",
    "            amplitude_rotation_diversity=metrics.get(\"amplitude_rotation_diversity\"),\n",
    "            allow_tableau=allow_tableau,\n",
    "            max_memory=max_memory,\n",
    "            max_time=max_time,\n",
    "            estimator=estimator,\n",
    "        )\n",
    "        if backend is None:\n",
    "            raise RuntimeError(\"fragment infeasible under the selected limits\")\n",
    "        selections.append((backend, diag[\"selected_cost\"]))\n",
    "        diagnostics.append(diag)\n",
    "\n",
    "    if boundaries:\n",
    "        plan = aggregate_partitioned_plan(selections, boundaries, estimator=estimator)\n",
    "        total_cost = plan[\"total_cost\"]\n",
    "        conversions = plan[\"conversions\"]\n",
    "    else:\n",
    "        total_cost = aggregate_single_backend_plan(selections)\n",
    "        conversions = []\n",
    "    return {\n",
    "        \"fragments\": diagnostics,\n",
    "        \"selections\": selections,\n",
    "        \"total_cost\": total_cost,\n",
    "        \"conversions\": conversions,\n",
    "    }\n",
    "\n",
    "\n",
    "def plan_overview(label: str, plan: dict) -> pd.DataFrame:\n",
    "    cost = plan[\"total_cost\"]\n",
    "    backends = \" → \".join(selection[0].name for selection in plan[\"selections\"])\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"plan\": label,\n",
    "                \"backends\": backends,\n",
    "                \"total_time\": cost.time,\n",
    "                \"peak_memory\": cost.memory,\n",
    "                \"conversion_time\": cost.conversion,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def fragment_breakdown(plan: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for idx, (backend, cost) in enumerate(plan[\"selections\"]):\n",
    "        diag = plan[\"fragments\"][idx]\n",
    "        metrics = diag.get(\"metrics\", {}) if isinstance(diag, dict) else {}\n",
    "        stats = diag.get(\"stats\") if isinstance(diag, dict) else None\n",
    "\n",
    "        def _lookup(field: str):\n",
    "            if stats is not None and hasattr(stats, field):\n",
    "                return getattr(stats, field)\n",
    "            return metrics.get(field)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"fragment\": idx,\n",
    "                \"backend\": backend.name,\n",
    "                \"num_qubits\": _lookup(\"num_qubits\"),\n",
    "                \"num_1q_gates\": _lookup(\"num_1q_gates\"),\n",
    "                \"num_2q_gates\": _lookup(\"num_2q_gates\"),\n",
    "                \"num_gates\": metrics.get(\"num_gates\"),\n",
    "                \"num_measurements\": _lookup(\"num_measurements\"),\n",
    "                \"sparsity\": metrics.get(\"sparsity\"),\n",
    "                \"is_clifford\": _lookup(\"is_clifford\"),\n",
    "                \"time\": cost.time,\n",
    "                \"memory\": cost.memory,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def conversion_breakdown(plan: dict) -> pd.DataFrame:\n",
    "    if not plan[\"conversions\"]:\n",
    "        return pd.DataFrame(columns=[\"index\", \"source\", \"target\", \"primitive\", \"time\", \"memory\"])\n",
    "    rows = []\n",
    "    for entry in plan[\"conversions\"]:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"index\": entry[\"index\"],\n",
    "                \"source\": entry[\"source\"].name,\n",
    "                \"target\": entry[\"target\"].name,\n",
    "                \"primitive\": entry[\"primitive\"],\n",
    "                \"time\": entry[\"cost\"].time,\n",
    "                \"memory\": entry[\"cost\"].memory,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def evaluate_partition_advantage(\n",
    "    gate_mixes: Sequence[float],\n",
    "    boundary_qubits: Sequence[int],\n",
    "    ranks: Sequence[int],\n",
    "    *,\n",
    "    total_qubits: int = 34,\n",
    "    total_depth: int = 72,\n",
    "    local_threshold: float = 0.32,\n",
    "    estimator: CostEstimator | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Map when a two-fragment plan beats a monolithic execution.\"\"\"\n",
    "\n",
    "    estimator = estimator or CostEstimator()\n",
    "    rows: list[dict] = []\n",
    "    depth_a = int(round(total_depth * 0.55))\n",
    "    depth_b = total_depth - depth_a\n",
    "\n",
    "    for gate_mix, boundary, rank in itertools.product(gate_mixes, boundary_qubits, ranks):\n",
    "        monolithic = synthesise_fragment(total_qubits, total_depth, gate_mix, is_local=False)\n",
    "        mono_backend, mono_diag = evaluate_fragment_backends(\n",
    "            monolithic,\n",
    "            sparsity=max(0.35, 1.0 - gate_mix * 1.4),\n",
    "            estimator=estimator,\n",
    "        )\n",
    "        mono_cost = mono_diag[\"selected_cost\"]\n",
    "\n",
    "        local_flag = gate_mix <= local_threshold\n",
    "        if local_flag:\n",
    "            frag_a = synthesise_fragment(\n",
    "                18,\n",
    "                depth_a,\n",
    "                max(0.08, gate_mix * 0.55),\n",
    "                is_local=True,\n",
    "                frontier_scale=0.24,\n",
    "                chi=64,\n",
    "            )\n",
    "            sparsity_a = min(0.95, 0.8 + (0.26 - gate_mix) * 1.5)\n",
    "        else:\n",
    "            frag_a = synthesise_fragment(\n",
    "                total_qubits,\n",
    "                depth_a,\n",
    "                gate_mix * 0.9,\n",
    "                is_local=False,\n",
    "            )\n",
    "            sparsity_a = max(0.35, 1.0 - gate_mix * 1.1)\n",
    "\n",
    "        frag_b = synthesise_fragment(total_qubits, depth_b, gate_mix, is_local=False)\n",
    "        sparsity_b = max(0.35, 1.0 - gate_mix * 1.3)\n",
    "\n",
    "        sel_a, diag_a = evaluate_fragment_backends(\n",
    "            frag_a,\n",
    "            sparsity=sparsity_a,\n",
    "            estimator=estimator,\n",
    "        )\n",
    "        sel_b, diag_b = evaluate_fragment_backends(\n",
    "            frag_b,\n",
    "            sparsity=sparsity_b,\n",
    "            estimator=estimator,\n",
    "        )\n",
    "\n",
    "        boundary_spec = BoundarySpec(\n",
    "            num_qubits=boundary,\n",
    "            rank=rank,\n",
    "            frontier=max(boundary, 12),\n",
    "            window=min(10, boundary // 2 + 2),\n",
    "            window_1q_gates=boundary * 4,\n",
    "            window_2q_gates=boundary * 2,\n",
    "        )\n",
    "\n",
    "        plan = aggregate_partitioned_plan(\n",
    "            [(sel_a, diag_a[\"selected_cost\"]), (sel_b, diag_b[\"selected_cost\"])],\n",
    "            [boundary_spec],\n",
    "            estimator=estimator,\n",
    "        )\n",
    "        total = plan[\"total_cost\"]\n",
    "        primitive = plan[\"conversions\"][0][\"primitive\"] if plan[\"conversions\"] else \"None\"\n",
    "        rows.append(\n",
    "            {\n",
    "                \"gate_mix\": gate_mix,\n",
    "                \"boundary_qubits\": boundary,\n",
    "                \"rank\": rank,\n",
    "                \"fragment_a_backend\": sel_a.name,\n",
    "                \"fragment_b_backend\": sel_b.name,\n",
    "                \"monolithic_backend\": mono_backend.name if mono_backend else None,\n",
    "                \"partition_time\": total.time,\n",
    "                \"monolithic_time\": mono_cost.time,\n",
    "                \"speedup\": mono_cost.time / total.time if total.time else float(\"inf\"),\n",
    "                \"primitive\": primitive,\n",
    "                \"partition_wins\": total.time < mono_cost.time,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af478d0",
   "metadata": {},
   "source": [
    "## Parameter grid (edit me)\n",
    "\n",
    "The dictionaries below define the parameter sweep for the feasibility study.  Update the lists to explore different fragment sizes, sparsities or resource ceilings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbeaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fragment_axes = {\n",
    "    \"num_qubits\": [18, 24, 30],\n",
    "    \"depth\": [48, 64],\n",
    "    \"entangling_ratio\": [0.12, 0.18, 0.28, 0.38],\n",
    "    \"measurement_ratio\": [0.0],\n",
    "    \"is_clifford\": [False, True],\n",
    "    \"is_local\": [False, True],\n",
    "    \"frontier\": [None],\n",
    "    \"frontier_scale\": [0.25],\n",
    "    \"chi\": [None],\n",
    "}\n",
    "metric_axes = {\n",
    "    \"sparsity\": np.linspace(0.55, 0.9, 4),\n",
    "    \"phase_rotation_diversity\": [6],\n",
    "    \"amplitude_rotation_diversity\": [8],\n",
    "}\n",
    "resource_limits = {\"max_memory_gb\": 64, \"max_time_s\": None}\n",
    "ALLOW_TABLEAU = True\n",
    "ESTIMATOR = CostEstimator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72eedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = evaluate_parameter_grid(\n",
    "    fragment_axes,\n",
    "    metric_axes,\n",
    "    resource_limits,\n",
    "    allow_tableau=ALLOW_TABLEAU,\n",
    "    estimator=ESTIMATOR,\n",
    ")\n",
    "grid_results[\"log_selected_time\"] = safe_log10(grid_results[\"selected_time\"])\n",
    "grid_results[\"log_selected_memory\"] = safe_log10(grid_results[\"selected_memory\"])\n",
    "grid_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b32cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "CELL_SUMMARY_MODE = \"mode_backend\"  # Choices: \"mode_backend\", \"majority_backend\", \"mean_runtime\"\n",
    "SUMMARY_LABELS = {\n",
    "    \"mode_backend\": \"Most common backend\",\n",
    "    \"majority_backend\": \"Backend majority (>50%)\",\n",
    "    \"mean_runtime\": \"Mean log10 runtime\",\n",
    "}\n",
    "if CELL_SUMMARY_MODE not in SUMMARY_LABELS:\n",
    "    raise ValueError(\n",
    "        f\"CELL_SUMMARY_MODE must be one of {', '.join(SUMMARY_LABELS)}, got {CELL_SUMMARY_MODE!r}\"\n",
    "    )\n",
    "\n",
    "preferred_backend_order = [\n",
    "    \"STATEVECTOR\",\n",
    "    \"DECISION_DIAGRAM\",\n",
    "    \"TABLEAU\",\n",
    "    \"MPS\",\n",
    "    \"TN\",\n",
    "    \"HYBRID\",\n",
    "    \"CUSTOM\",\n",
    "]\n",
    "color_lookup = {\n",
    "    \"STATEVECTOR\": \"#4477aa\",\n",
    "    \"DECISION_DIAGRAM\": \"#66c2a5\",\n",
    "    \"TABLEAU\": \"#8dd3c7\",\n",
    "    \"MPS\": \"#ffa600\",\n",
    "    \"TN\": \"#aa3377\",\n",
    "    \"HYBRID\": \"#ccbb44\",\n",
    "    \"CUSTOM\": \"#7f7f7f\",\n",
    "    \"MIXED\": \"#bdbdbd\",\n",
    "}\n",
    "fallback_colors = [\"#a1d99b\", \"#984ea3\", \"#f781bf\", \"#999999\"]\n",
    "\n",
    "selected_depth = _to_iterable(fragment_axes[\"depth\"])[0]\n",
    "subset = grid_results[grid_results[\"depth\"] == selected_depth].copy()\n",
    "if \"phase_rotation_diversity\" in subset and not subset[\"phase_rotation_diversity\"].isna().all():\n",
    "    subset = subset[\n",
    "        subset[\"phase_rotation_diversity\"]\n",
    "        == _to_iterable(metric_axes[\"phase_rotation_diversity\"])[0]\n",
    "    ]\n",
    "if \"amplitude_rotation_diversity\" in subset and not subset[\"amplitude_rotation_diversity\"].isna().all():\n",
    "    subset = subset[\n",
    "        subset[\"amplitude_rotation_diversity\"]\n",
    "        == _to_iterable(metric_axes[\"amplitude_rotation_diversity\"])[0]\n",
    "    ]\n",
    "\n",
    "sweep_columns = sorted({*fragment_axes.keys(), *metric_axes.keys()})\n",
    "sweep_columns = [col for col in sweep_columns if col in subset.columns]\n",
    "\n",
    "\n",
    "def _mode_or_first(series: pd.Series):\n",
    "    cleaned = series.dropna()\n",
    "    if cleaned.empty:\n",
    "        return np.nan\n",
    "    modes = cleaned.mode()\n",
    "    if not modes.empty:\n",
    "        return modes.iloc[0]\n",
    "    return cleaned.iloc[0]\n",
    "\n",
    "\n",
    "def _log_value(value: float) -> float:\n",
    "    if value is None or not np.isfinite(value):\n",
    "        return np.nan\n",
    "    return float(safe_log10([value])[0])\n",
    "\n",
    "\n",
    "if sweep_columns:\n",
    "    aggregated = (\n",
    "        subset.groupby(sweep_columns, dropna=False)\n",
    "        .agg(\n",
    "            selected_backend=(\"selected_backend\", _mode_or_first),\n",
    "            selected_time=(\"selected_time\", \"mean\"),\n",
    "            selected_memory=(\"selected_memory\", \"mean\"),\n",
    "            log_selected_time=(\"log_selected_time\", \"mean\"),\n",
    "            log_selected_memory=(\"log_selected_memory\", \"mean\"),\n",
    "            num_1q_gates=(\"num_1q_gates\", \"first\"),\n",
    "            num_2q_gates=(\"num_2q_gates\", \"first\"),\n",
    "            num_measurements=(\"num_measurements\", \"first\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "else:\n",
    "    aggregated = subset.copy()\n",
    "\n",
    "\n",
    "def _summarise_variants(group: pd.DataFrame) -> pd.Series:\n",
    "    backend_series = group[\"selected_backend\"].dropna().astype(str)\n",
    "    counts = backend_series.value_counts()\n",
    "    total = int(counts.sum())\n",
    "    if total:\n",
    "        top_backend = counts.index[0]\n",
    "        top_count = int(counts.iloc[0])\n",
    "        majority_backend = top_backend if top_count > total / 2 else \"MIXED\"\n",
    "        majority_fraction = top_count / total\n",
    "    else:\n",
    "        top_backend = np.nan\n",
    "        majority_backend = np.nan\n",
    "        majority_fraction = np.nan\n",
    "    runtime_series = group[\"selected_time\"].dropna().astype(float)\n",
    "    if runtime_series.empty:\n",
    "        mean_runtime = np.nan\n",
    "        median_runtime = np.nan\n",
    "    else:\n",
    "        mean_runtime = runtime_series.mean()\n",
    "        median_runtime = runtime_series.median()\n",
    "    mean_log_runtime = _log_value(mean_runtime) if np.isfinite(mean_runtime) else np.nan\n",
    "    median_log_runtime = _log_value(median_runtime) if np.isfinite(median_runtime) else np.nan\n",
    "    log_series = group[\"log_selected_time\"].dropna().astype(float)\n",
    "    if log_series.size > 1:\n",
    "        runtime_std = log_series.std(ddof=0)\n",
    "    elif log_series.size == 1:\n",
    "        runtime_std = 0.0\n",
    "    else:\n",
    "        runtime_std = np.nan\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"mode_backend\": top_backend if total else np.nan,\n",
    "            \"majority_backend\": majority_backend if total else np.nan,\n",
    "            \"majority_fraction\": majority_fraction,\n",
    "            \"mean_runtime\": mean_runtime,\n",
    "            \"median_runtime\": median_runtime,\n",
    "            \"mean_log_runtime\": mean_log_runtime,\n",
    "            \"median_log_runtime\": median_log_runtime,\n",
    "            \"runtime_std\": runtime_std,\n",
    "            \"num_variants\": total,\n",
    "            \"unique_backends\": len(counts),\n",
    "            \"backend_labels\": tuple(counts.index.tolist()) if total else tuple(),\n",
    "            \"tableau_fraction\": (float(counts.get(\"TABLEAU\", 0)) / total) if total else np.nan,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "if aggregated.empty:\n",
    "    cell_summary = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"is_local\",\n",
    "            \"num_qubits\",\n",
    "            \"sparsity\",\n",
    "            \"mode_backend\",\n",
    "            \"majority_backend\",\n",
    "            \"majority_fraction\",\n",
    "            \"mean_runtime\",\n",
    "            \"median_runtime\",\n",
    "            \"mean_log_runtime\",\n",
    "            \"median_log_runtime\",\n",
    "            \"runtime_std\",\n",
    "            \"num_variants\",\n",
    "            \"unique_backends\",\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    cell_summary = (\n",
    "        aggregated.groupby([\"is_local\", \"num_qubits\", \"sparsity\"], dropna=False)\n",
    "        .apply(_summarise_variants)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "summary_column_map = {\n",
    "    \"mode_backend\": \"mode_backend\",\n",
    "    \"majority_backend\": \"majority_backend\",\n",
    "    \"mean_runtime\": \"mean_log_runtime\",\n",
    "}\n",
    "summary_column = summary_column_map[CELL_SUMMARY_MODE]\n",
    "backend_mode = CELL_SUMMARY_MODE in {\"mode_backend\", \"majority_backend\"}\n",
    "\n",
    "runtime_values = cell_summary.get(\"mean_log_runtime\")\n",
    "if runtime_values is not None:\n",
    "    runtime_values = runtime_values.to_numpy(dtype=float)\n",
    "    runtime_values = runtime_values[np.isfinite(runtime_values)]\n",
    "else:\n",
    "    runtime_values = np.array([])\n",
    "time_vmin = runtime_values.min() if runtime_values.size else None\n",
    "time_vmax = runtime_values.max() if runtime_values.size else None\n",
    "\n",
    "if backend_mode:\n",
    "    backend_values = cell_summary.get(summary_column, pd.Series(dtype=object)).dropna().astype(str)\n",
    "    backend_values = backend_values[backend_values.ne(\"\")]\n",
    "    observed_backends = list(dict.fromkeys(backend_values))\n",
    "    backend_order = [b for b in preferred_backend_order if b in observed_backends]\n",
    "    backend_order.extend([b for b in observed_backends if b not in backend_order and b != \"MIXED\"])\n",
    "    if \"MIXED\" in observed_backends:\n",
    "        backend_order.append(\"MIXED\")\n",
    "    if not backend_order:\n",
    "        backend_order = [\"STATEVECTOR\"]\n",
    "    colors = []\n",
    "    fallback_pool = list(fallback_colors)\n",
    "    for backend in backend_order:\n",
    "        color = color_lookup.get(backend)\n",
    "        if color is None:\n",
    "            color = fallback_pool.pop(0) if fallback_pool else \"#cccccc\"\n",
    "        colors.append(color)\n",
    "    backend_cmap = ListedColormap(colors)\n",
    "    backend_norm = mpl.colors.BoundaryNorm(\n",
    "        np.arange(len(backend_order) + 1) - 0.5, backend_cmap.N\n",
    "    )\n",
    "    backend_to_idx = {name: idx for idx, name in enumerate(backend_order)}\n",
    "else:\n",
    "    backend_order = []\n",
    "    backend_cmap = None\n",
    "    backend_norm = None\n",
    "    backend_to_idx = None\n",
    "\n",
    "\n",
    "def _heatmap_payload(summary: pd.DataFrame, value_column: str, *, index_lookup: dict[str, int] | None = None, label_column: str | None = None):\n",
    "    if summary.empty:\n",
    "        if label_column:\n",
    "            return [], [], np.empty((0, 0)), []\n",
    "        return [], [], np.empty((0, 0))\n",
    "    qubits = sorted(summary[\"num_qubits\"].unique())\n",
    "    sparsities = sorted(summary[\"sparsity\"].unique())\n",
    "    data = np.full((len(sparsities), len(qubits)), np.nan)\n",
    "    labels = [[tuple() for _ in qubits] for _ in sparsities] if label_column else None\n",
    "    for i, sparsity in enumerate(sparsities):\n",
    "        for j, nq in enumerate(qubits):\n",
    "            row = summary[\n",
    "                (summary[\"num_qubits\"] == nq) & (summary[\"sparsity\"] == sparsity)\n",
    "            ]\n",
    "            if row.empty:\n",
    "                continue\n",
    "            entry = row.iloc[0]\n",
    "            value = entry[value_column]\n",
    "            if label_column:\n",
    "                raw_labels = entry.get(label_column)\n",
    "                if isinstance(raw_labels, float) and np.isnan(raw_labels):\n",
    "                    label_value = tuple()\n",
    "                elif raw_labels is None:\n",
    "                    label_value = tuple()\n",
    "                elif isinstance(raw_labels, (list, tuple, set)):\n",
    "                    label_value = tuple(raw_labels)\n",
    "                else:\n",
    "                    label_value = (str(raw_labels),)\n",
    "                labels[i][j] = label_value\n",
    "            if index_lookup is None:\n",
    "                data[i, j] = value\n",
    "            else:\n",
    "                if pd.isna(value):\n",
    "                    continue\n",
    "                key = str(value)\n",
    "                if key in index_lookup:\n",
    "                    data[i, j] = index_lookup[key]\n",
    "    if labels is None:\n",
    "        return qubits, sparsities, data\n",
    "    return qubits, sparsities, data, labels\n",
    "\n",
    "\n",
    "titles = {False: \"Distributed fragments\", True: \"Local fragments\"}\n",
    "\n",
    "fig_backend, backend_axes = plt.subplots(1, 2, figsize=(12, 4.5), sharex=True, sharey=True)\n",
    "cell_backend_labels = {}\n",
    "backend_axes = np.atleast_1d(backend_axes)\n",
    "backend_im = None\n",
    "\n",
    "for ax_idx, is_local in enumerate([False, True]):\n",
    "    frame = cell_summary[cell_summary[\"is_local\"] == is_local]\n",
    "    if backend_mode:\n",
    "        payload = _heatmap_payload(\n",
    "            frame, summary_column, index_lookup=backend_to_idx, label_column=\"backend_labels\"\n",
    "        )\n",
    "        qubits, sparsities, matrix, label_grid = payload\n",
    "    else:\n",
    "        payload = _heatmap_payload(frame, summary_column, index_lookup=backend_to_idx)\n",
    "        qubits, sparsities, matrix = payload\n",
    "        label_grid = None\n",
    "    ax = backend_axes[ax_idx]\n",
    "    if not qubits or not sparsities:\n",
    "        ax.axis(\"off\")\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        continue\n",
    "    if backend_mode:\n",
    "        masked = np.ma.masked_invalid(matrix)\n",
    "        backend_im = ax.imshow(\n",
    "            masked,\n",
    "            cmap=backend_cmap,\n",
    "            norm=backend_norm,\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "    else:\n",
    "        masked = np.ma.masked_invalid(matrix)\n",
    "        backend_im = ax.imshow(\n",
    "            masked,\n",
    "            cmap=\"viridis\",\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "            vmin=time_vmin,\n",
    "            vmax=time_vmax,\n",
    "        )\n",
    "    if backend_mode and label_grid:\n",
    "        for i, sparsity in enumerate(sparsities):\n",
    "            for j, nq in enumerate(qubits):\n",
    "                cell_backend_labels[(is_local, nq, sparsity)] = label_grid[i][j]\n",
    "    ax.set_title(titles[is_local], fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xticks(range(len(qubits)))\n",
    "    ax.set_xticklabels(qubits)\n",
    "    ax.set_yticks(range(len(sparsities)))\n",
    "    ax.set_yticklabels([f\"{val:.2f}\" for val in sparsities])\n",
    "    if ax_idx == 0:\n",
    "        ax.set_ylabel(\"sparsity\")\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_xlabel(\"fragment qubits\")\n",
    "\n",
    "if backend_im is not None:\n",
    "    if backend_mode:\n",
    "        cbar_backend = fig_backend.colorbar(\n",
    "            mpl.cm.ScalarMappable(cmap=backend_cmap, norm=backend_norm),\n",
    "            ax=backend_axes,\n",
    "            orientation=\"horizontal\",\n",
    "            pad=0.15,\n",
    "        )\n",
    "        cbar_backend.set_label(SUMMARY_LABELS[CELL_SUMMARY_MODE])\n",
    "        cbar_backend.set_ticks(range(len(backend_order)))\n",
    "        cbar_backend.set_ticklabels(backend_order)\n",
    "    else:\n",
    "        cbar_backend = fig_backend.colorbar(\n",
    "            backend_im,\n",
    "            ax=backend_axes,\n",
    "            orientation=\"horizontal\",\n",
    "            pad=0.15,\n",
    "        )\n",
    "        cbar_backend.set_label(SUMMARY_LABELS[CELL_SUMMARY_MODE])\n",
    "fig_backend.suptitle(\n",
    "    f\"{SUMMARY_LABELS[CELL_SUMMARY_MODE]} across the parameter sweep\", fontsize=14\n",
    ")\n",
    "fig_backend.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "fig_runtime, runtime_axes = plt.subplots(1, 2, figsize=(12, 4.5), sharex=True, sharey=True)\n",
    "runtime_axes = np.atleast_1d(runtime_axes)\n",
    "runtime_im = None\n",
    "\n",
    "for ax_idx, is_local in enumerate([False, True]):\n",
    "    frame = cell_summary[cell_summary[\"is_local\"] == is_local]\n",
    "    qubits, sparsities, log_time = _heatmap_payload(frame, \"mean_log_runtime\")\n",
    "    ax = runtime_axes[ax_idx]\n",
    "    if not qubits or not sparsities:\n",
    "        ax.axis(\"off\")\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        continue\n",
    "    masked_runtime = np.ma.masked_invalid(log_time)\n",
    "    runtime_im = ax.imshow(\n",
    "        masked_runtime,\n",
    "        cmap=\"viridis\",\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        vmin=time_vmin,\n",
    "        vmax=time_vmax,\n",
    "    )\n",
    "    ax.set_title(titles[is_local], fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xticks(range(len(qubits)))\n",
    "    ax.set_xticklabels(qubits)\n",
    "    ax.set_yticks(range(len(sparsities)))\n",
    "    ax.set_yticklabels([f\"{val:.2f}\" for val in sparsities])\n",
    "    if ax_idx == 0:\n",
    "        ax.set_ylabel(\"sparsity\")\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_xlabel(\"fragment qubits\")\n",
    "\n",
    "if runtime_im is not None:\n",
    "    cbar_runtime = fig_runtime.colorbar(\n",
    "        runtime_im,\n",
    "        ax=runtime_axes,\n",
    "        orientation=\"horizontal\",\n",
    "        pad=0.15,\n",
    "    )\n",
    "    cbar_runtime.set_label(\"Mean log10 runtime estimate\")\n",
    "fig_runtime.suptitle(\n",
    "    \"Mean log10 runtime across the parameter sweep\", fontsize=14\n",
    ")\n",
    "fig_runtime.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "tableau_variants = aggregated[aggregated[\"selected_backend\"] == \"TABLEAU\"].copy()\n",
    "if not tableau_variants.empty:\n",
    "    print(\"Tableau-winning parameter combinations (aggregated view):\")\n",
    "    if cell_backend_labels:\n",
    "        tableau_variants[\"cell_backends\"] = tableau_variants.apply(\n",
    "            lambda row: cell_backend_labels.get(\n",
    "                (row[\"is_local\"], row[\"num_qubits\"], row[\"sparsity\"]), tuple()\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        tableau_variants[\"cell_backends\"] = tableau_variants[\"cell_backends\"].apply(\n",
    "            lambda labels: \" → \".join(labels) if labels else \"\"\n",
    "        )\n",
    "    tableau_variants[\"log10_time\"] = safe_log10(tableau_variants[\"selected_time\"])\n",
    "    tableau_variants[\"log10_memory\"] = safe_log10(tableau_variants[\"selected_memory\"])\n",
    "    tableau_columns = [\n",
    "        \"is_local\",\n",
    "        \"is_clifford\",\n",
    "        \"num_qubits\",\n",
    "        \"depth\",\n",
    "        \"entangling_ratio\",\n",
    "        \"sparsity\",\n",
    "        \"num_1q_gates\",\n",
    "        \"num_2q_gates\",\n",
    "        \"num_measurements\",\n",
    "        \"selected_backend\",\n",
    "        \"selected_time\",\n",
    "        \"log10_time\",\n",
    "        \"selected_memory\",\n",
    "        \"log10_memory\",\n",
    "        \"cell_backends\",\n",
    "    ]\n",
    "    display(\n",
    "        tableau_variants[tableau_columns]\n",
    "        .sort_values([\"is_local\", \"is_clifford\", \"num_qubits\", \"entangling_ratio\", \"sparsity\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    print(\"Tableau did not win for any sampled configuration in this sweep.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b181cb",
   "metadata": {},
   "source": [
    "### Backend feasibility maps\n",
    "\n",
    "The heatmaps aggregate every combination of swept parameters before drawing a cell. All fragment variants that share the same size, sparsity and locality are grouped across depth, entangling ratio and the remaining sweep dimensions. The `CELL_SUMMARY_MODE` toggle controls the summary shown per cell: \"mode_backend\" displays the most common backend, \"majority_backend\" only labels a backend when it wins a strict majority (otherwise the cell is marked as `MIXED`), and \"mean_runtime\" renders the mean log10 runtime for the aggregated variants. Switching between these options highlights when the omitted dimensions drive different backend choices or materially change the runtime estimates.\n",
    "\n",
    "A summary table beneath the figures highlights the parameter combinations where the Tableau simulator prevails, including the gate counts contributed by each fragment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c11010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "CELL_SUMMARY_MODE = \"mode_backend\"  # Choices: \"mode_backend\", \"majority_backend\", \"mean_runtime\"\n",
    "SUMMARY_LABELS = {\n",
    "    \"mode_backend\": \"Most common backend\",\n",
    "    \"majority_backend\": \"Backend majority (>50%)\",\n",
    "    \"mean_runtime\": \"Mean log10 runtime\",\n",
    "}\n",
    "if CELL_SUMMARY_MODE not in SUMMARY_LABELS:\n",
    "    raise ValueError(\n",
    "        f\"CELL_SUMMARY_MODE must be one of {', '.join(SUMMARY_LABELS)}, got {CELL_SUMMARY_MODE!r}\"\n",
    "    )\n",
    "\n",
    "preferred_backend_order = [\n",
    "    \"STATEVECTOR\",\n",
    "    \"DECISION_DIAGRAM\",\n",
    "    \"MPS\",\n",
    "    \"TN\",\n",
    "    \"HYBRID\",\n",
    "    \"CUSTOM\",\n",
    "]\n",
    "color_lookup = {\n",
    "    \"STATEVECTOR\": \"#4477aa\",\n",
    "    \"DECISION_DIAGRAM\": \"#66c2a5\",\n",
    "    \"MPS\": \"#ffa600\",\n",
    "    \"TN\": \"#aa3377\",\n",
    "    \"HYBRID\": \"#ccbb44\",\n",
    "    \"CUSTOM\": \"#7f7f7f\",\n",
    "    \"MIXED\": \"#bdbdbd\",\n",
    "}\n",
    "fallback_colors = [\"#a1d99b\", \"#984ea3\", \"#f781bf\", \"#999999\"]\n",
    "\n",
    "selected_depth = _to_iterable(fragment_axes[\"depth\"])[0]\n",
    "subset = grid_results[grid_results[\"depth\"] == selected_depth].copy()\n",
    "if \"phase_rotation_diversity\" in subset and not subset[\"phase_rotation_diversity\"].isna().all():\n",
    "    subset = subset[\n",
    "        subset[\"phase_rotation_diversity\"]\n",
    "        == _to_iterable(metric_axes[\"phase_rotation_diversity\"])[0]\n",
    "    ]\n",
    "if \"amplitude_rotation_diversity\" in subset and not subset[\"amplitude_rotation_diversity\"].isna().all():\n",
    "    subset = subset[\n",
    "        subset[\"amplitude_rotation_diversity\"]\n",
    "        == _to_iterable(metric_axes[\"amplitude_rotation_diversity\"])[0]\n",
    "    ]\n",
    "\n",
    "sweep_columns = sorted({*fragment_axes.keys(), *metric_axes.keys()})\n",
    "sweep_columns = [col for col in sweep_columns if col in subset.columns]\n",
    "\n",
    "\n",
    "def _mode_or_first(series: pd.Series):\n",
    "    cleaned = series.dropna()\n",
    "    if cleaned.empty:\n",
    "        return np.nan\n",
    "    modes = cleaned.mode()\n",
    "    if not modes.empty:\n",
    "        return modes.iloc[0]\n",
    "    return cleaned.iloc[0]\n",
    "\n",
    "\n",
    "def _log_value(value: float) -> float:\n",
    "    if value is None or not np.isfinite(value):\n",
    "        return np.nan\n",
    "    return float(safe_log10([value])[0])\n",
    "\n",
    "\n",
    "if sweep_columns:\n",
    "    aggregated = (\n",
    "        subset.groupby(sweep_columns, dropna=False)\n",
    "        .agg(\n",
    "            selected_backend=(\"selected_backend\", _mode_or_first),\n",
    "            selected_time=(\"selected_time\", \"mean\"),\n",
    "            log_selected_time=(\"log_selected_time\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "else:\n",
    "    aggregated = subset.copy()\n",
    "\n",
    "\n",
    "def _summarise_variants(group: pd.DataFrame) -> pd.Series:\n",
    "    backend_series = group[\"selected_backend\"].dropna().astype(str)\n",
    "    counts = backend_series.value_counts()\n",
    "    total = int(counts.sum())\n",
    "    if total:\n",
    "        top_backend = counts.index[0]\n",
    "        top_count = int(counts.iloc[0])\n",
    "        majority_backend = top_backend if top_count > total / 2 else \"MIXED\"\n",
    "        majority_fraction = top_count / total\n",
    "    else:\n",
    "        top_backend = np.nan\n",
    "        majority_backend = np.nan\n",
    "        majority_fraction = np.nan\n",
    "    runtime_series = group[\"selected_time\"].dropna().astype(float)\n",
    "    if runtime_series.empty:\n",
    "        mean_runtime = np.nan\n",
    "        median_runtime = np.nan\n",
    "    else:\n",
    "        mean_runtime = runtime_series.mean()\n",
    "        median_runtime = runtime_series.median()\n",
    "    mean_log_runtime = _log_value(mean_runtime) if np.isfinite(mean_runtime) else np.nan\n",
    "    median_log_runtime = _log_value(median_runtime) if np.isfinite(median_runtime) else np.nan\n",
    "    log_series = group[\"log_selected_time\"].dropna().astype(float)\n",
    "    if log_series.size > 1:\n",
    "        runtime_std = log_series.std(ddof=0)\n",
    "    elif log_series.size == 1:\n",
    "        runtime_std = 0.0\n",
    "    else:\n",
    "        runtime_std = np.nan\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"mode_backend\": top_backend if total else np.nan,\n",
    "            \"majority_backend\": majority_backend if total else np.nan,\n",
    "            \"majority_fraction\": majority_fraction,\n",
    "            \"mean_runtime\": mean_runtime,\n",
    "            \"median_runtime\": median_runtime,\n",
    "            \"mean_log_runtime\": mean_log_runtime,\n",
    "            \"median_log_runtime\": median_log_runtime,\n",
    "            \"runtime_std\": runtime_std,\n",
    "            \"num_variants\": total,\n",
    "            \"unique_backends\": len(counts),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "if aggregated.empty:\n",
    "    cell_summary = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"is_local\",\n",
    "            \"num_qubits\",\n",
    "            \"sparsity\",\n",
    "            \"mode_backend\",\n",
    "            \"majority_backend\",\n",
    "            \"majority_fraction\",\n",
    "            \"mean_runtime\",\n",
    "            \"median_runtime\",\n",
    "            \"mean_log_runtime\",\n",
    "            \"median_log_runtime\",\n",
    "            \"runtime_std\",\n",
    "            \"num_variants\",\n",
    "            \"unique_backends\",\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    cell_summary = (\n",
    "        aggregated.groupby([\"is_local\", \"num_qubits\", \"sparsity\"], dropna=False)\n",
    "        .apply(_summarise_variants)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "summary_column_map = {\n",
    "    \"mode_backend\": \"mode_backend\",\n",
    "    \"majority_backend\": \"majority_backend\",\n",
    "    \"mean_runtime\": \"mean_log_runtime\",\n",
    "}\n",
    "summary_column = summary_column_map[CELL_SUMMARY_MODE]\n",
    "backend_mode = CELL_SUMMARY_MODE in {\"mode_backend\", \"majority_backend\"}\n",
    "\n",
    "runtime_values = cell_summary.get(\"mean_log_runtime\")\n",
    "if runtime_values is not None:\n",
    "    runtime_values = runtime_values.to_numpy(dtype=float)\n",
    "    runtime_values = runtime_values[np.isfinite(runtime_values)]\n",
    "else:\n",
    "    runtime_values = np.array([])\n",
    "time_vmin = runtime_values.min() if runtime_values.size else None\n",
    "time_vmax = runtime_values.max() if runtime_values.size else None\n",
    "\n",
    "if backend_mode:\n",
    "    backend_values = cell_summary.get(summary_column, pd.Series(dtype=object)).dropna().astype(str)\n",
    "    backend_values = backend_values[backend_values.ne(\"TABLEAU\") & backend_values.ne(\"\")]\n",
    "    observed_backends = list(dict.fromkeys(backend_values))\n",
    "    backend_order = [b for b in preferred_backend_order if b in observed_backends]\n",
    "    backend_order.extend([b for b in observed_backends if b not in backend_order and b != \"MIXED\"])\n",
    "    if \"MIXED\" in observed_backends:\n",
    "        backend_order.append(\"MIXED\")\n",
    "    if not backend_order:\n",
    "        backend_order = [\"STATEVECTOR\"]\n",
    "    colors = []\n",
    "    fallback_pool = list(fallback_colors)\n",
    "    for backend in backend_order:\n",
    "        color = color_lookup.get(backend)\n",
    "        if color is None:\n",
    "            color = fallback_pool.pop(0) if fallback_pool else \"#cccccc\"\n",
    "        colors.append(color)\n",
    "    backend_cmap = ListedColormap(colors)\n",
    "    backend_norm = mpl.colors.BoundaryNorm(\n",
    "        np.arange(len(backend_order) + 1) - 0.5, backend_cmap.N\n",
    "    )\n",
    "    backend_to_idx = {name: idx for idx, name in enumerate(backend_order)}\n",
    "else:\n",
    "    backend_order = []\n",
    "    backend_cmap = None\n",
    "    backend_norm = None\n",
    "    backend_to_idx = None\n",
    "\n",
    "\n",
    "def _heatmap_payload(summary: pd.DataFrame, value_column: str, *, index_lookup: dict[str, int] | None = None):\n",
    "    if summary.empty:\n",
    "        return [], [], np.empty((0, 0))\n",
    "    qubits = sorted(summary[\"num_qubits\"].unique())\n",
    "    sparsities = sorted(summary[\"sparsity\"].unique())\n",
    "    data = np.full((len(sparsities), len(qubits)), np.nan)\n",
    "    for i, sparsity in enumerate(sparsities):\n",
    "        for j, nq in enumerate(qubits):\n",
    "            row = summary[\n",
    "                (summary[\"num_qubits\"] == nq) & (summary[\"sparsity\"] == sparsity)\n",
    "            ]\n",
    "            if row.empty:\n",
    "                continue\n",
    "            value = row.iloc[0][value_column]\n",
    "            if index_lookup is None:\n",
    "                data[i, j] = value\n",
    "            else:\n",
    "                if pd.isna(value):\n",
    "                    continue\n",
    "                key = str(value)\n",
    "                if key in index_lookup:\n",
    "                    data[i, j] = index_lookup[key]\n",
    "    return qubits, sparsities, data\n",
    "\n",
    "\n",
    "titles = {False: \"Distributed fragments\", True: \"Local fragments\"}\n",
    "\n",
    "fig_backend, backend_axes = plt.subplots(1, 2, figsize=(12, 4.5), sharex=True, sharey=True)\n",
    "backend_axes = np.atleast_1d(backend_axes)\n",
    "backend_im = None\n",
    "\n",
    "for ax_idx, is_local in enumerate([False, True]):\n",
    "    frame = cell_summary[cell_summary[\"is_local\"] == is_local]\n",
    "    qubits, sparsities, matrix = _heatmap_payload(\n",
    "        frame, summary_column, index_lookup=backend_to_idx\n",
    "    )\n",
    "    ax = backend_axes[ax_idx]\n",
    "    if not qubits or not sparsities:\n",
    "        ax.axis(\"off\")\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        continue\n",
    "    if backend_mode:\n",
    "        masked = np.ma.masked_invalid(matrix)\n",
    "        backend_im = ax.imshow(\n",
    "            masked,\n",
    "            cmap=backend_cmap,\n",
    "            norm=backend_norm,\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "    else:\n",
    "        masked = np.ma.masked_invalid(matrix)\n",
    "        backend_im = ax.imshow(\n",
    "            masked,\n",
    "            cmap=\"viridis\",\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "            vmin=time_vmin,\n",
    "            vmax=time_vmax,\n",
    "        )\n",
    "    ax.set_title(titles[is_local], fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xticks(range(len(qubits)))\n",
    "    ax.set_xticklabels(qubits)\n",
    "    ax.set_yticks(range(len(sparsities)))\n",
    "    ax.set_yticklabels([f\"{val:.2f}\" for val in sparsities])\n",
    "    if ax_idx == 0:\n",
    "        ax.set_ylabel(\"sparsity\")\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_xlabel(\"fragment qubits\")\n",
    "\n",
    "if backend_im is not None:\n",
    "    if backend_mode:\n",
    "        cbar_backend = fig_backend.colorbar(\n",
    "            mpl.cm.ScalarMappable(cmap=backend_cmap, norm=backend_norm),\n",
    "            ax=backend_axes,\n",
    "            orientation=\"horizontal\",\n",
    "            pad=0.15,\n",
    "        )\n",
    "        cbar_backend.set_label(SUMMARY_LABELS[CELL_SUMMARY_MODE])\n",
    "        cbar_backend.set_ticks(range(len(backend_order)))\n",
    "        cbar_backend.set_ticklabels(backend_order)\n",
    "    else:\n",
    "        cbar_backend = fig_backend.colorbar(\n",
    "            backend_im,\n",
    "            ax=backend_axes,\n",
    "            orientation=\"horizontal\",\n",
    "            pad=0.15,\n",
    "        )\n",
    "        cbar_backend.set_label(SUMMARY_LABELS[CELL_SUMMARY_MODE])\n",
    "fig_backend.suptitle(\n",
    "    f\"{SUMMARY_LABELS[CELL_SUMMARY_MODE]} across the parameter sweep\", fontsize=14\n",
    ")\n",
    "fig_backend.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "fig_runtime, runtime_axes = plt.subplots(1, 2, figsize=(12, 4.5), sharex=True, sharey=True)\n",
    "runtime_axes = np.atleast_1d(runtime_axes)\n",
    "runtime_im = None\n",
    "\n",
    "for ax_idx, is_local in enumerate([False, True]):\n",
    "    frame = cell_summary[cell_summary[\"is_local\"] == is_local]\n",
    "    qubits, sparsities, log_time = _heatmap_payload(frame, \"mean_log_runtime\")\n",
    "    ax = runtime_axes[ax_idx]\n",
    "    if not qubits or not sparsities:\n",
    "        ax.axis(\"off\")\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        continue\n",
    "    masked_runtime = np.ma.masked_invalid(log_time)\n",
    "    runtime_im = ax.imshow(\n",
    "        masked_runtime,\n",
    "        cmap=\"viridis\",\n",
    "        aspect=\"auto\",\n",
    "        origin=\"lower\",\n",
    "        vmin=time_vmin,\n",
    "        vmax=time_vmax,\n",
    "    )\n",
    "    ax.set_title(titles[is_local], fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xticks(range(len(qubits)))\n",
    "    ax.set_xticklabels(qubits)\n",
    "    ax.set_yticks(range(len(sparsities)))\n",
    "    ax.set_yticklabels([f\"{val:.2f}\" for val in sparsities])\n",
    "    if ax_idx == 0:\n",
    "        ax.set_ylabel(\"sparsity\")\n",
    "    else:\n",
    "        ax.set_yticklabels([])\n",
    "    ax.set_xlabel(\"fragment qubits\")\n",
    "\n",
    "if runtime_im is not None:\n",
    "    cbar_runtime = fig_runtime.colorbar(\n",
    "        runtime_im,\n",
    "        ax=runtime_axes,\n",
    "        orientation=\"horizontal\",\n",
    "        pad=0.15,\n",
    "    )\n",
    "    cbar_runtime.set_label(\"Mean log10 runtime estimate\")\n",
    "fig_runtime.suptitle(\n",
    "    \"Mean log10 runtime across the parameter sweep\", fontsize=14\n",
    ")\n",
    "fig_runtime.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47acd5",
   "metadata": {},
   "source": [
    "### Case study: two segments with a decision diagram prefix\n",
    "\n",
    "A sparse, locally entangling prefix can often be simulated efficiently with the decision-diagram backend before converting into a dense statevector section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_case1 = {\"max_memory_gb\": 64}\n",
    "monolithic_stats = synthesise_fragment(34, 70, 0.35, is_local=False)\n",
    "monolithic_plan = run_plan([monolithic_stats], [{\"sparsity\": 0.62}], resource_limits=resource_case1, estimator=ESTIMATOR)\n",
    "\n",
    "fragment_a = synthesise_fragment(18, 44, 0.18, is_local=True, frontier_scale=0.25, chi=48)\n",
    "fragment_b = synthesise_fragment(34, 32, 0.32, is_local=False)\n",
    "boundary_ab = BoundarySpec(num_qubits=14, rank=48, frontier=20, window=8, window_1q_gates=60, window_2q_gates=16)\n",
    "partition_plan = run_plan(\n",
    "    [fragment_a, fragment_b],\n",
    "    [{\"sparsity\": 0.88}, {\"sparsity\": 0.58}],\n",
    "    boundaries=[boundary_ab],\n",
    "    resource_limits=resource_case1,\n",
    "    estimator=ESTIMATOR,\n",
    ")\n",
    "\n",
    "overview = pd.concat(\n",
    "    [\n",
    "        plan_overview(\"Monolithic statevector\", monolithic_plan),\n",
    "        plan_overview(\"DD → statevector\", partition_plan),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "overview[\"time_speedup_vs_monolithic\"] = overview.loc[0, \"total_time\"] / overview[\"total_time\"]\n",
    "overview[\"memory_ratio_vs_monolithic\"] = overview.loc[0, \"peak_memory\"] / overview[\"peak_memory\"]\n",
    "display(overview)\n",
    "\n",
    "display(fragment_breakdown(partition_plan))\n",
    "conversion_df = conversion_breakdown(partition_plan)\n",
    "if not conversion_df.empty:\n",
    "    display(conversion_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axes[0].bar(overview[\"plan\"], safe_log10(overview[\"total_time\"]))\n",
    "axes[0].set_ylabel(\"log10 total time\")\n",
    "axes[1].bar(overview[\"plan\"], safe_log10(overview[\"peak_memory\"]))\n",
    "axes[1].set_ylabel(\"log10 peak memory\")\n",
    "plt.suptitle(\"Two-segment plan compared to the monolithic execution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6e16b",
   "metadata": {},
   "source": [
    "### Case study: three segments with conversions\n",
    "\n",
    "A Clifford initialisation, a local MPS window and a dense finale illustrate how multiple conversions accumulate while still reducing projected runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_case2 = {\"max_memory_gb\": 128}\n",
    "mono_stats = synthesise_fragment(48, 90, 0.32, is_local=False)\n",
    "mono_plan = run_plan([mono_stats], [{\"sparsity\": 0.58}], resource_limits=resource_case2, estimator=ESTIMATOR)\n",
    "\n",
    "frag1 = synthesise_fragment(48, 18, 0.0, is_clifford=True, is_local=False)\n",
    "frag2 = synthesise_fragment(32, 50, 0.18, is_local=True, frontier_scale=0.2, chi=48)\n",
    "frag3 = synthesise_fragment(48, 36, 0.34, is_local=False)\n",
    "\n",
    "boundary_12 = BoundarySpec(num_qubits=12, rank=32, frontier=24, window=6, window_1q_gates=40, window_2q_gates=12)\n",
    "boundary_23 = BoundarySpec(num_qubits=18, rank=64, frontier=30, window=8, window_1q_gates=60, window_2q_gates=20)\n",
    "three_plan = run_plan(\n",
    "    [frag1, frag2, frag3],\n",
    "    [{\"sparsity\": 0.95}, {\"sparsity\": 0.88}, {\"sparsity\": 0.55}],\n",
    "    boundaries=[boundary_12, boundary_23],\n",
    "    resource_limits=resource_case2,\n",
    "    estimator=ESTIMATOR,\n",
    ")\n",
    "\n",
    "overview_three = pd.concat(\n",
    "    [\n",
    "        plan_overview(\"Monolithic statevector\", mono_plan),\n",
    "        plan_overview(\"Tableau → MPS → statevector\", three_plan),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "overview_three[\"time_speedup_vs_monolithic\"] = overview_three.loc[0, \"total_time\"] / overview_three[\"total_time\"]\n",
    "overview_three[\"memory_ratio_vs_monolithic\"] = overview_three.loc[0, \"peak_memory\"] / overview_three[\"peak_memory\"]\n",
    "display(overview_three)\n",
    "\n",
    "display(fragment_breakdown(three_plan))\n",
    "conv_three = conversion_breakdown(three_plan)\n",
    "if not conv_three.empty:\n",
    "    display(conv_three)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "axes[0].bar(overview_three[\"plan\"], safe_log10(overview_three[\"total_time\"]))\n",
    "axes[0].set_ylabel(\"log10 total time\")\n",
    "axes[1].bar(overview_three[\"plan\"], safe_log10(overview_three[\"peak_memory\"]))\n",
    "axes[1].set_ylabel(\"log10 peak memory\")\n",
    "plt.suptitle(\"Three-segment plan compared to the monolithic execution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc52d7",
   "metadata": {},
   "source": [
    "### Tableau snapshot for the Clifford fragment\n",
    "\n",
    "The three-segment case study begins with a Clifford initialisation that is scheduled on the Tableau simulator. The table below compares its gate counts and projected cost with the subsequent fragments in the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_labels = [\"Clifford prefix\", \"Local tensor window\", \"Dense conclusion\"]\n",
    "fragment_stats_seq = [frag1, frag2, frag3]\n",
    "segment_rows = []\n",
    "for label, stats, selection in zip(segment_labels, fragment_stats_seq, three_plan[\"selections\"]):\n",
    "    backend, cost = selection\n",
    "    segment_rows.append(\n",
    "        {\n",
    "            \"segment\": label,\n",
    "            \"backend\": backend.name,\n",
    "            \"is_clifford\": getattr(stats, \"is_clifford\", False),\n",
    "            \"num_qubits\": getattr(stats, \"num_qubits\", None),\n",
    "            \"num_1q_gates\": getattr(stats, \"num_1q_gates\", None),\n",
    "            \"num_2q_gates\": getattr(stats, \"num_2q_gates\", None),\n",
    "            \"num_measurements\": getattr(stats, \"num_measurements\", None),\n",
    "            \"time\": cost.time,\n",
    "            \"memory\": cost.memory,\n",
    "        }\n",
    "    )\n",
    "clifford_profile = pd.DataFrame(segment_rows)\n",
    "clifford_profile[\"log10_time\"] = safe_log10(clifford_profile[\"time\"])\n",
    "clifford_profile[\"log10_memory\"] = safe_log10(clifford_profile[\"memory\"])\n",
    "display(clifford_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e0fd8",
   "metadata": {},
   "source": [
    "### Feature map for partition advantage\n",
    "\n",
    "The following sweep varies the gate mix (fraction of entangling layers), conversion boundary width and Schmidt-rank cap.  It highlights where the model predicts a win for the two-fragment plan and which conversion primitive is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_df = evaluate_partition_advantage(\n",
    "    gate_mixes=[0.22, 0.28, 0.34, 0.4],\n",
    "    boundary_qubits=[8, 12, 16, 20, 24, 28],\n",
    "    ranks=[16, 32, 64],\n",
    "    estimator=ESTIMATOR,\n",
    ")\n",
    "partition_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200682d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage = partition_df[partition_df[\"partition_wins\"]].copy()\n",
    "losses = partition_df[~partition_df[\"partition_wins\"]].copy()\n",
    "\n",
    "summary = (\n",
    "    advantage.groupby([\"gate_mix\", \"primitive\"])\n",
    "    .agg(\n",
    "        min_boundary=(\"boundary_qubits\", \"min\"),\n",
    "        max_speedup=(\"speedup\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"gate_mix\", \"min_boundary\"])\n",
    ")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not losses.empty:\n",
    "    loss_summary = (\n",
    "        losses.groupby([\"gate_mix\", \"primitive\"])\n",
    "        .agg(\n",
    "            min_boundary=(\"boundary_qubits\", \"min\"),\n",
    "            max_boundary=(\"boundary_qubits\", \"max\"),\n",
    "            worst_speedup=(\"speedup\", \"min\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"gate_mix\", \"min_boundary\"])\n",
    "    )\n",
    "    display(loss_summary)\n",
    "else:\n",
    "    print(\"Partitioning won for every sampled configuration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e74681",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(sorted(partition_df[\"rank\"].unique())), figsize=(15, 3), sharey=True)\n",
    "for ax, rank in zip(np.atleast_1d(axes), sorted(partition_df[\"rank\"].unique())):\n",
    "    view = partition_df[partition_df[\"rank\"] == rank]\n",
    "    pivot = view.pivot_table(index=\"gate_mix\", columns=\"boundary_qubits\", values=\"speedup\", aggfunc=\"mean\")\n",
    "    im = ax.imshow(np.log10(pivot.values), cmap=\"viridis\", aspect=\"auto\", origin=\"lower\")\n",
    "    ax.set_title(f\"rank ≤ {rank}\")\n",
    "    ax.set_xticks(range(len(pivot.columns)), pivot.columns)\n",
    "    ax.set_yticks(range(len(pivot.index)), [f\"{v:.2f}\" for v in pivot.index])\n",
    "    ax.set_xlabel(\"boundary qubits\")\n",
    "axes[0].set_ylabel(\"gate mix (entangling fraction)\")\n",
    "fig.colorbar(im, ax=axes, orientation=\"horizontal\", fraction=0.04, pad=0.1, label=\"log10 speedup\")\n",
    "plt.suptitle(\"Speedup heatmap across boundary sizes and gate mixes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0db13",
   "metadata": {},
   "source": [
    "The remaining tables characterise losing configurations and conversion choices.  When the initial fragment is forced onto the dense statevector backend, the conversions disappear and the partition offers no benefit, underscoring the importance of sparsity or locality for heterogeneous plans."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
